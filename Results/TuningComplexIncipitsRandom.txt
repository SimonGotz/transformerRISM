Run 0, config: lr: 2e-06 wd: 0.1 d_model: 16 n_heads: 4 n_layers: 2 d_ff: 256 batch_size: 512 dropout: 0.39452751804838804 margin: 0.630187536593566 epsilon: 0.0001 Train losses: 0.7704960778355598 0.7732329182326794 0.7749363631010056 0.7718633338809013 0.7747169733047485 0.7410214506089687 0.767705861479044 0.7669361084699631 0.7920551598072052 0.7650086171925068 0.747346505522728 0.766901783645153 0.7807428054511547 0.7603899352252483 0.7672841809689999 0.7823707051575184 0.7857527323067188 0.7592443265020847 0.7780828401446342 0.7601895444095135 0.7572518885135651 0.7676945738494396 0.7607135735452175 0.7792389504611492 0.7728942446410656 Val losses: 0.4279737174510956 0.3909643292427063 0.4001791675885518 0.4296781023343404 0.40953967968622845 0.392427255709966 0.4055188298225403 0.41921087106068927 0.40022096037864685 0.4036502540111542 0.4186867872873942 0.40151212612787884 0.4298727214336395 0.3852890531222026 0.4144983192284902 0.4285123248895009 0.4116908510526021 0.41954754789670307 0.41756781935691833 0.437582661708196 0.4294792016347249 0.4082983632882436 0.4345677097638448 0.38801323374112445 0.4177379906177521 Testloss: 0.4286430908627667 
Run 1, config: lr: 2e-05 wd: 0.0001 d_model: 512 n_heads: 16 n_layers: 3 d_ff: 512 batch_size: 16 dropout: 0.0886794625587048 margin: 0.7736612671467831 epsilon: 1e-05 Train losses: 0.4192686759763294 0.39861438715899433 0.3693030701743232 0.32596208442140506 0.29742226512343795 0.2608463986052407 0.2389655003945033 0.22313939542682082 0.21309645771980285 0.19835496346155804 0.1938243493989662 0.18424453547707312 0.17624747311627423 0.17410662163186957 0.17099595290643196 0.15238672693570454 0.16074291533894008 0.15554484658771092 0.15432413187291888 0.14525793923272026 0.14690410373387514 0.1427460233370463 0.14362364775604672 0.1441357120319649 0.14462771537127317 Val losses: 0.36780713692955347 0.3533564707507258 0.3301936538323112 0.3365225460218347 0.320961833777635 0.3167281363321387 0.32602849835934844 0.27892167360886283 0.3011366468408833 0.28026986808880516 0.28301562304082123 0.250228866805201 0.2729139177695565 0.2500663962053216 0.2625346961228744 0.281719622404679 0.26975779300150665 0.2667424036108929 0.2589367407819499 0.24692646394605222 0.25438314209813656 0.23517947067385134 0.2471741824046425 0.2639847869458406 0.23176289485848467 Testloss: 0.2875607826463729 
Run 2, config: lr: 2e-06 wd: 0.01 d_model: 128 n_heads: 16 n_layers: 1 d_ff: 128 batch_size: 64 dropout: 0.35349726857331265 margin: 0.13527575067186282 epsilon: 1e-07 Train losses: 0.46050421330663893 0.46624237254813866 0.4625695080668838 0.46579947626149215 0.45727409256829155 0.4640085112165522 0.45388632350497776 0.459027314958749 0.45484931468963624 0.4515147105411247 0.4508564414801421 0.4644015098059619 0.4572388216301247 0.44446701032144054 0.4477216686363573 0.44007505068072567 0.4429398229828587 0.44343094141395006 0.453727157579528 0.44921602805455524 0.4500129211831976 0.4504288313565431 0.46031463499422426 0.44939405377264374 Val losses: 0.1956280448607036 0.20036002221916402 0.17324060494346277 0.20385094013597285 0.20329828480524675 0.1922720582889659 0.18784346617758274 0.18664326279291085 0.17889283996607577 0.186175974085927 0.19536463410726615 0.1946976219436952 0.18049168506903307 0.1887753467474665 0.18023358870829856 0.2037292452795165 0.19939380963998182 0.19201187177428178 0.1917635646781751 0.19393931355859553 0.19111040819968497 0.1923944660063301 0.19496696309319564 0.19586191007069179 Testloss: 0.18894372908634988 
Run 3, config: lr: 1e-05 wd: 0.05 d_model: 128 n_heads: 4 n_layers: 3 d_ff: 256 batch_size: 128 dropout: 0.57938226211107 margin: 0.9382317686031046 epsilon: 0.01 Train losses: 1.0325847454925081 1.0274123407121916 1.026555061340332 1.035852131558888 1.009873930198043 1.019153880539225 1.0283038785208518 1.0313802646167243 1.0167988707770164 1.015315047840574 1.008503761754107 1.0026524574009341 1.004075427553547 1.014281628736809 1.001609826265876 1.0318125290657156 1.0289751113350711 1.0404946981970944 1.0327875961118669 1.003365530006921 1.0232536721585401 1.0102678332755815 1.0060562082191011 1.0132538729639196 1.0165733561586978 Val losses: 0.530377100620951 0.5358911156654358 0.526781793151583 0.5461881714207786 0.5222468972206116 0.5260855853557587 0.5067173604454313 0.5332195652382714 0.4990892005818231 0.4836894529206412 0.5045114989791598 0.5331714898347855 0.5328322244541985 0.5280750053269523 0.526208181466375 0.53375315453325 0.5324784006391253 0.5289338103362492 0.5044905458177839 0.4996250867843628 0.5586395753281457 0.5255788202796664 0.5484574926751 0.54571590253285 0.5080440725599017 Testloss: 0.5200716957152051 
Run 4, config: lr: 2e-05 wd: 0.1 d_model: 512 n_heads: 16 n_layers: 2 d_ff: 256 batch_size: 1024 dropout: 0.12070951275478192 margin: 0.6749766021666693 epsilon: 1e-08 Train losses: 0.4331844598054886 0.43783941492438316 0.4302448481321335 0.42028411477804184 0.4101978838443756 0.3761592246592045 0.3927192762494087 0.3830995336174965 0.3824640028178692 0.3766140341758728 0.3699296973645687 0.3604215085506439 0.36454689502716064 0.347203329205513 0.34165482223033905 0.3415299132466316 0.3406611420214176 0.34123721718788147 0.34971221163868904 0.3474685475230217 0.3374391235411167 0.3372919075191021 0.33702919632196426 0.335533507168293 0.3373912237584591 Val losses: 0.41327759623527527 0.41681429743766785 0.3999134600162506 0.356703519821167 0.38621917366981506 0.3762608468532562 0.3638307750225067 0.3535043001174927 0.38284212350845337 0.339438796043396 0.3445105254650116 0.3947050869464874 0.36054593324661255 0.3647981286048889 0.3630736470222473 0.3868645131587982 0.3532312512397766 0.38457974791526794 0.4312668442726135 0.3526915907859802 0.3553442358970642 0.33236172795295715 0.38012659549713135 0.3535040616989136 0.3443983495235443 Testloss: 0.3185933005932149 
Run 5, config: lr: 2e-05 wd: 0.05 d_model: 16 n_heads: 16 n_layers: 1 d_ff: 256 batch_size: 128 dropout: 0.6615174215374081 margin: 0.5836501704381452 epsilon: 0.1 Train losses: 0.8198207376608208 0.8116853904368272 0.8222868771695379 0.8151441456666634 0.8125942972168994 0.8158682077678282 0.8041687803481942 0.8058804220228053 0.8033929938700661 0.8157643996067901 0.8039266289170108 0.7915776274097499 0.7909094762446275 0.8016939990556062 0.8111378772934871 0.8004929218719254 0.8166369653459805 0.8192732058354278 0.8016344691390422 0.8257564421909959 0.8139721306402292 0.8128162426735038 0.8106520318273288 0.8188564697308327 0.8017700999530394 Val losses: 0.3700254112482071 0.3670854036297117 0.36841041701180594 0.3689864192690168 0.365873851946422 0.3634036204644612 0.3712524473667145 0.3615761974028179 0.3686790487595967 0.3611590287515095 0.36575169009821756 0.3678673931530544 0.35849456063338686 0.3749287745782307 0.3835677738700594 0.3637245148420334 0.38049362174102236 0.35851690598896574 0.3612152487039566 0.35494575117315563 0.35423209198883604 0.3704310804605484 0.3674928694963455 0.3591801460300173 0.35003796219825745 Testloss: 0.34840498062032804 
Run 6, config: lr: 0.0001 wd: 0.1 d_model: 256 n_heads: 4 n_layers: 4 d_ff: 512 batch_size: 16 dropout: 0.5429275732971682 margin: 0.8748677123419862 epsilon: 0.0001 Train losses: 0.9153325511349573 0.9000266014425843 0.8745713314524404 0.8022504072498392 0.6921003467506832 0.6284841950292941 0.5969236956702338 0.5729327085945342 0.5532042926108396 0.5506391732781022 0.5399885868584668 0.5330532377516782 0.5115838321270766 0.506420671387955 0.48589851370564213 0.48902273961791287 0.4862181300366366 0.4713670148893639 0.48686337228174564 0.4734566582573785 0.48700782976768636 0.46384664431766226 0.4658075827139395 0.4636493776683454 0.47345784935686325 Val losses: 0.46416809792103975 0.41631386642870694 0.431133838062701 0.44655456776204316 0.6846596393896186 0.7404670107623805 0.6622267666070357 0.6577335124430449 0.6789300433967425 0.7027828763360563 0.6298183078351228 0.6965864479541779 0.6878639971432479 0.6404238682726155 0.6451517362957415 0.5585414614366448 0.6411853752706362 0.580657898472703 0.5869798373916875 0.6681981735903284 0.6054830374925033 0.6005090033230575 0.5365666114765665 0.5615961875604547 0.6136588750973991 Testloss: 0.4800771041408479 
Run 7, config: lr: 2e-06 wd: 0.0001 d_model: 16 n_heads: 2 n_layers: 2 d_ff: 1024 batch_size: 256 dropout: 0.49320954948535434 margin: 0.6819095888620433 epsilon: 0.1 Train losses: 0.8803648388747013 0.8816199483293475 0.8646429289471019 0.8642551591902068 0.8737697222016074 0.8689978754881656 0.8735720713933309 0.87966172803532 0.890852538022128 0.8706098108580618 0.8644670537023833 0.8712354717832623 0.8941949533693718 0.8784324490662777 0.8897927656318202 0.8548272017276648 0.8968242768085364 0.8917650742964311 0.8796664115154382 0.8669948559818845 0.886181784398628 0.8615042549191099 Val losses: 0.40978492157799856 0.4122669739382608 0.44090893864631653 0.4324746642793928 0.4134146400860378 0.436135538986751 0.4547668397426605 0.4166914565222604 0.4268904115472521 0.4142594635486603 0.41875178047588896 0.3955442862851279 0.43812223417418344 0.4300260373524257 0.40828279086521696 0.42388842361313955 0.4424351836953844 0.42070609756878447 0.39900737575122286 0.4079395404883793 0.4235805230481284 0.4328454179423196 Testloss: 0.4244004166622152 
Run 8, config: lr: 2e-06 wd: 0.005 d_model: 1024 n_heads: 2 n_layers: 4 d_ff: 1024 batch_size: 64 dropout: 0.6438444783510546 margin: 0.2068858527134509 epsilon: 1e-05 Train losses: 0.48713050683339437 0.49211638194543345 0.49897853047759444 0.4948861832971926 0.5000859136934633 0.4885713193151686 0.4995405276616414 0.49209367522486935 0.5003585497538249 0.4929663587499548 0.4942205367264924 0.48449596299065484 0.4967576018086186 0.4911559621493022 0.5012476651756852 0.4975094627451014 0.487316358089447 0.4890281774379589 0.4864636010593838 0.4937088745611685 0.49536968778680873 0.49206331394336844 0.486953858534495 0.4745518578423394 0.48013064287326956 Val losses: 0.5375826529094151 0.45004851850015776 0.4406356178224087 0.48463630782706396 0.49362748754876 0.5016430887792792 0.48068997051034656 0.5607528558799199 0.4736690047596182 0.4765486397913524 0.39920896557824953 0.4734356052109173 0.4951836402927126 0.4788905105420521 0.5228772392230374 0.44266544708183836 0.5111832395195961 0.5097494258412293 0.4859703869691917 0.4785847956580775 0.4723495878279209 0.5476528847856181 0.45935370879513876 0.4265581002192838 0.4888674483767578 Testloss: 0.4561159908288495 
Run 9, config: lr: 0.001 wd: 0.1 d_model: 128 n_heads: 16 n_layers: 3 d_ff: 512 batch_size: 32 dropout: 0.19569545071740224 margin: 0.40937148673558144 epsilon: 0.0001 Train losses: 0.3715029251796228 0.24892250299453736 0.2045986275430079 0.18420671555731033 0.16256309057827348 0.14261851252781022 0.12990349235909956 0.11842462503247791 0.10574894707511973 0.10340711697936059 0.10023814268686153 0.09362153126685707 0.08694565011947243 0.08559028594582169 0.08149240891690608 0.07591575044724676 0.07381635738743676 0.0670441803280954 0.06845753521279052 0.06932886171120184 0.06153215608663029 0.06226551074672628 0.06540467258956698 0.05851784679624769 0.053436582231963126 Val losses: 0.22817086546044602 0.22163756492367961 0.22028965816686027 0.21099161801108143 0.21165708949168524 0.20239709265399397 0.17275839913309665 0.21244191692063683 0.1685627649227778 0.17117333595167128 0.1547224258905963 0.17527944293984196 0.1474637591786552 0.17439476439827367 0.15726259755983688 0.14728145209843652 0.1466256072908117 0.145695618500835 0.15041230658167287 0.14626850004781755 0.12731705998119555 0.14309719936889514 0.13163445458600395 0.14871616504694285 0.11809486412165458 Testloss: 0.11821164690694701 
Run 10, config: lr: 0.001 wd: 0.01 d_model: 256 n_heads: 16 n_layers: 2 d_ff: 1024 batch_size: 256 dropout: 0.36561686564957396 margin: 0.2828951125890815 epsilon: 1e-08 Train losses: 0.47703108101180103 0.3411617396455823 0.27169264582070435 0.21813327390136142 0.198085192026514 0.18054487578796619 0.16807634699525256 0.15795467974561633 0.14995662077809824 0.14806145745696445 0.1352234369877613 0.14059061805407205 0.13212523157849457 0.12886026704853232 0.11971428967786557 0.12420702861113982 0.11578159959930362 0.11959228845256747 0.11471692743626508 0.11236756453008363 0.10846229766805966 0.11052326180718162 0.1045385892644073 0.10772735580350414 0.10356823287226936 Val losses: 0.2219025194644928 0.39901173966271536 0.3737294716494424 0.35294403774397715 0.2904143737895148 0.29680057082857403 0.25766595559460775 0.22963860843862807 0.23908301975045884 0.218602254986763 0.27479378453322817 0.23163205172334397 0.24109056804861342 0.22775758164269583 0.23258498736790248 0.23471741591181075 0.23005822513784682 0.24908049404621124 0.20343573604311263 0.22173665038176946 0.22837557750088827 0.21245259046554565 0.23952691682747432 0.2363527970654624 0.19555516753877913 Testloss: 0.1768888233160084 
Run 11, config: lr: 2e-06 wd: 0.05 d_model: 128 n_heads: 2 n_layers: 1 d_ff: 2048 batch_size: 128 dropout: 0.7771274000895283 margin: 0.6138173583552057 epsilon: 0.001 Train losses: 0.8654538606529805 0.8409938207313196 0.8365579888002196 0.8457282763808521 0.82157218278344 0.8489241066263683 0.854859499788996 0.8434575822815966 0.8434527258374798 0.8511007650574641 Val losses: 0.48136572752680096 0.4820185112101691 0.5102193802595139 0.47982126474380493 0.4970618209668568 0.510332533291408 0.4579636688743319 0.47451614694935934 0.49499673928533283 0.5014493486710957 Testloss: 0.5634290265721414 
Run 12, config: lr: 0.0001 wd: 0.0001 d_model: 1024 n_heads: 2 n_layers: 3 d_ff: 2048 batch_size: 128 dropout: 0.6744741434864476 margin: 0.11570748093904125 epsilon: 1e-05 Train losses: 0.46388017197153464 0.45751126533123987 0.45022548890825526 0.4234647212633446 0.41595079471815877 0.403530031887453 0.39002611788351144 0.3637524407301376 0.34056845129425845 Val losses: 0.3630715597953115 0.3675597033330372 0.3720362846340452 0.40140730142593384 0.5016229450702667 0.4615726109061922 0.6717392270054136 0.7405117579868862 0.9910335327897754 Testloss: 0.41151947642841064 
Run 13, config: lr: 1e-05 wd: 0.0005 d_model: 128 n_heads: 2 n_layers: 1 d_ff: 1024 batch_size: 128 dropout: 0.34795568470353955 margin: 0.6303532946412017 epsilon: 1e-07 Train losses: 0.7666510201212186 0.7789981338515211 0.7681216517491127 0.7659726605486514 0.7453567296711366 0.7492852575743376 0.7507536927265908 0.7471618011816225 0.7517902886689599 0.745091632230958 0.7476034689305434 0.729861347532984 0.7345411599572025 0.7361415108638023 0.7273775419192527 0.7410554254232947 0.7385974855565313 0.7366194244640977 Val losses: 0.5152365842035839 0.5260266235896519 0.510365007179124 0.5114747434854507 0.4668412378856114 0.4419819435903004 0.4641997792891094 0.4319961965084076 0.433583670428821 0.47299404442310333 0.45028847455978394 0.4624109502349581 0.4723266363143921 0.45675499737262726 0.4394502767494747 0.44813645098890575 0.44890459520476206 0.46208182190145763 Testloss: 0.48823025437833684 
Run 14, config: lr: 0.002 wd: 0.0001 d_model: 16 n_heads: 2 n_layers: 3 d_ff: 512 batch_size: 1024 dropout: 0.4786667360186163 margin: 0.3677566033206463 epsilon: 1e-05 Train losses: 0.5932174772024155 0.521338626742363 0.4658607058227062 0.4585004299879074 0.4389866292476654 0.4250909239053726 0.4099074602127075 0.41519855707883835 0.4046582318842411 0.3944702483713627 0.3946063928306103 0.3935740627348423 0.38838986679911613 0.38994090259075165 0.3870208151638508 0.37923211604356766 0.37943949177861214 0.35437555238604546 0.3553995229303837 Val losses: 0.24167659878730774 0.21335580945014954 0.25394174456596375 0.269750714302063 0.27876460552215576 0.2764943838119507 0.27835479378700256 0.28239452838897705 0.28218263387680054 0.26682934165000916 0.26176759600639343 0.2611392140388489 0.2590847909450531 0.24480685591697693 0.23669753968715668 0.2277492880821228 0.23270593583583832 0.24650301039218903 0.2576049864292145 Testloss: 0.21808022977950314 
Run 15, config: lr: 2e-06 wd: 0.0001 d_model: 512 n_heads: 8 n_layers: 2 d_ff: 1024 batch_size: 512 dropout: 0.670126994478161 margin: 0.15685754179800251 epsilon: 1e-06 Train losses: 0.5340088102966547 0.5181610491126776 0.5340833514928818 0.5305626466870308 0.5335663985460997 0.5315601415932178 0.5304232034832239 0.5306736547499895 0.5176413878798485 0.5364624597132206 0.5329424645751715 Val losses: 0.22658705711364746 0.21681687235832214 0.20841712752978006 0.25234899421532947 0.21962012350559235 0.2268119901418686 0.24134612083435059 0.2191354235013326 0.23317890365918478 0.24179048836231232 0.2512548665205638 Testloss: 0.22041683799830208 
Run 16, config: lr: 1e-05 wd: 0.0001 d_model: 16 n_heads: 2 n_layers: 1 d_ff: 128 batch_size: 256 dropout: 0.35184318928077335 margin: 0.293248029849271 epsilon: 0.001 Train losses: 0.597457797238321 0.6035127693956549 0.5900732640064124 0.5796544028050972 0.5998998139843796 0.5797686053044868 0.5848484707601143 0.5984867558334813 0.5963528535582803 0.5865843025120822 0.5834513125997601 0.5728739156867518 0.5896854400634766 0.5919687946637472 0.5765124324596289 0.5841458223082803 0.5861897540814949 0.5680590985399304 0.5696749172427438 0.5832219828258861 0.5869222689758647 0.5795182648933295 0.5746583803133531 0.5816423874912839 0.5774041874842211 Val losses: 0.2650753791843142 0.2523631772824696 0.2537489128964288 0.2532724546534674 0.2702659262078149 0.26692019615854534 0.2569781520536968 0.25191935683999744 0.2628033012151718 0.2592502406665257 0.24433467643601553 0.2423564110483442 0.26330011231558664 0.25027800670691897 0.2789119482040405 0.2660485953092575 0.25318718382290434 0.2510891322578703 0.2677385083266667 0.26488778633730753 0.2652758913380759 0.2613927147218159 0.26095091870852877 0.2561413624456951 0.24480313701289041 Testloss: 0.24785374848022132 
Run 17, config: lr: 1e-05 wd: 0.05 d_model: 32 n_heads: 16 n_layers: 4 d_ff: 2048 batch_size: 32 dropout: 0.7099591833933587 margin: 0.2885881533013473 epsilon: 1e-06 Train losses: 0.5979638318220775 0.581277538449676 0.5890423680345217 0.5529958820453397 0.5600840720865462 0.5427219556437598 0.5129271693803646 0.5112326678854447 0.5040543124631599 0.5027360864259579 0.4953440502837852 0.4883761113440549 0.4875547545927542 0.4804822729141624 0.47406003844958766 0.47208368800304557 0.4823380247310356 0.47768983598108644 0.4747510270939933 0.4691445906956991 0.46217488950049435 0.4653275568176199 0.4818914932785211 0.47210776000111193 0.47353106715061044 Val losses: 0.22467443163980516 0.20172933763579318 0.20814238567101329 0.21287858028683745 0.17582238086482935 0.17708311373727365 0.17594715848303677 0.17088069766759872 0.16480381917535214 0.15626288061602073 0.15887966686696336 0.1570334965199755 0.15297318642076693 0.1510412077370443 0.15521439504727982 0.15615907246083544 0.15555380376284583 0.15759876710281037 0.15668956778551402 0.1565432395589979 0.15790507877082155 0.162056059560232 0.1604297842111504 0.16107471823169475 0.1610983729101064 Testloss: 0.16132518339968577 
Run 18, config: lr: 0.001 wd: 0.1 d_model: 1024 n_heads: 16 n_layers: 2 d_ff: 256 batch_size: 128 dropout: 0.2641384885090866 margin: 0.7717265925324803 epsilon: 0.001 Train losses: 0.5994751364437502 0.5691393458131534 0.5039962387796658 0.4568361800108383 0.4515606765426807 0.4551022911249702 0.4278658535053481 0.39493680756483507 0.3893812111954191 0.3717234659550795 0.3767462486206596 0.368068124820937 0.36612670190298735 0.3523969946067725 0.34378792834815697 0.3568341909949459 0.3464671021966792 0.3320275111429727 0.3407390578024423 0.3383196409958512 0.33256456745204643 0.3322230321702673 0.33855595739919747 0.3325362579146428 0.32712618650785136 Val losses: 0.4620529647384371 0.5222551950386592 0.46551758689539774 0.48442902309553965 0.47085716681821005 0.48681105247565676 0.48806246902261463 0.48720282529081615 0.4026305909667696 0.43753765310559956 0.4335082301071712 0.388629396046911 0.37857783479349955 0.4335771862949644 0.37905709019729067 0.37699539746556965 0.3654668139559882 0.43826226783650263 0.36567688626902445 0.38727158520902905 0.3740840469087873 0.38395084760018755 0.4008182553308351 0.36498942758355823 0.3855140932968685 Testloss: 0.49958132795872784 
Run 19, config: lr: 1e-05 wd: 0.05 d_model: 16 n_heads: 4 n_layers: 1 d_ff: 2048 batch_size: 128 dropout: 0.631861425936829 margin: 0.8252090929965554 epsilon: 1e-07 Train losses: 0.9910643545549307 0.9700977677729592 0.9807947767314626 0.9607065257741444 0.9727040289053276 0.9457172021937015 0.9434688758494248 0.9496152294215872 0.9406331065875381 0.9308445738322699 0.9445490534625837 0.9580759904277858 0.9496545960654074 0.9494163269427285 0.9330620454318488 0.9394448782081035 0.9331257058613336 0.9351527584132864 0.9134676554309789 0.9345883718177453 0.9172450428578391 0.9401660312467547 0.9373605233519825 0.9325330221830909 0.9358976679061776 Val losses: 0.5392682360751289 0.5464678278991154 0.5191988476685115 0.5385633685759136 0.5369562315089362 0.5403833091259003 0.523944731269564 0.5505763675485339 0.5365016311407089 0.5213830066578728 0.5246520021132061 0.5140312782355717 0.5438463773046222 0.537656443459647 0.5297542299543109 0.5502030019249234 0.5167593955993652 0.5613189488649368 0.5525114919458117 0.5540582601513181 0.5350082069635391 0.5395058116742543 0.5234745634453637 0.5435697031872613 0.5502020631517682 Testloss: 0.5441989240808657 
Run 20, config: lr: 2e-06 wd: 0.005 d_model: 1024 n_heads: 8 n_layers: 4 d_ff: 128 batch_size: 256 dropout: 0.7310683984164851 margin: 0.6673911711895757 epsilon: 1e-08 Train losses: 0.8030663042357473 0.7954943938688799 0.7843509677684668 0.8037061980276397 0.7980496377655955 0.8050374804121075 0.8067259029908613 0.7871130452011571 0.8074881849866925 0.7900372635234486 Val losses: 0.433553478547505 0.4573203665869577 0.4295001881463187 0.45560117278780254 0.42457745330674307 0.3805015981197357 0.3759738164288657 0.3917195626667568 0.4136922061443329 0.4225480854511261 Testloss: 0.52868204255931 
Run 21, config: lr: 1e-05 wd: 0.05 d_model: 64 n_heads: 8 n_layers: 2 d_ff: 2048 batch_size: 32 dropout: 0.4265937788152583 margin: 0.31333624579332087 epsilon: 1e-07 Train losses: 0.6017255201935768 0.5799442215098275 0.5584866907861498 0.5278320938348771 0.5105647846504494 0.4871838630901443 0.4614425606749676 0.45577431111424055 0.4545527042614089 0.44095445347053036 0.4456663080387645 0.43813498202297424 0.4286489087122458 0.43264590744619014 0.42695442106988696 0.435575683746073 0.4219787127993725 0.42807036196744 0.4246251627250954 0.4200608066386647 0.42498381314454253 0.4240684722860654 0.4291353836655617 0.42294507633756706 0.42116631065253857 Val losses: 0.23778751727781797 0.23866353092486398 0.22011512243434003 0.2185162227404745 0.21249625361279437 0.2125978609710409 0.17424564821678296 0.17672462532656236 0.18146403918140813 0.16894693162880445 0.18237705071244323 0.16510663678248724 0.15850228986196352 0.15533610649014773 0.17281869916539444 0.16262729426747874 0.163572219903009 0.16899485598530686 0.1556883732739248 0.157321729877016 0.16527425210204041 0.15970411073220403 0.1683294307767299 0.16612662531827627 0.16245691439038829 Testloss: 0.15625882264667432 
Run 22, config: lr: 0.0002 wd: 0.0005 d_model: 32 n_heads: 8 n_layers: 3 d_ff: 256 batch_size: 256 dropout: 0.5310990241851034 margin: 0.4845426698807686 epsilon: 0.1 Train losses: 0.7030838691827023 0.689192553361257 0.6972097898974563 0.6842145504373492 0.692980372544491 0.6916277408599854 0.688473667159225 0.6870778401692709 0.6965684547568812 0.6987733660322247 0.6825238231456641 0.6794740467360525 0.7023262309305596 0.6902798071052089 0.7000842419537631 0.6777125885992339 0.670142796906558 0.6807019475734595 0.6895574439655651 0.7063934261148627 0.692977867343209 0.6958001472733237 0.6956514625838308 0.6965997815132141 0.6762452685471737 Val losses: 0.34732131021363394 0.34416578497205463 0.3589786503996168 0.35681005886622835 0.33222445845603943 0.3476180391652243 0.33405056595802307 0.3477001360484532 0.3277808555534908 0.33638266154697966 0.35365488273756845 0.3447950482368469 0.34329141889299664 0.36534727045467924 0.340824442250388 0.3476253662790571 0.35001172763960703 0.34210646578243803 0.34021569575582233 0.36654206684657503 0.3511826864310673 0.33761936000415255 0.33156403473445345 0.3174606263637543 0.337477513722011 Testloss: 0.3095052464468166 
Run 23, config: lr: 2e-05 wd: 0.0005 d_model: 64 n_heads: 8 n_layers: 4 d_ff: 1024 batch_size: 512 dropout: 0.10954404909368805 margin: 0.7999761741022153 epsilon: 0.1 Train losses: 0.6083545312285423 0.5944953188300133 0.6057978868484497 0.5972749888896942 0.6067934967577457 0.6122674196958542 0.6018958389759064 0.6000098660588264 0.6117293983697891 0.6112666577100754 0.600932102650404 0.6027909703552723 0.5955629423260689 0.6058032773435116 Val losses: 0.4653229018052419 0.46603379646937054 0.4490775565306346 0.45837069551150006 0.47414591908454895 0.46538009246190387 0.47001978754997253 0.4289775987466176 0.46747560302416485 0.4496437907218933 0.44055033723513287 0.46021708846092224 0.48103274901707965 0.48114057381947833 Testloss: 0.4302138272393143 
Run 24, config: lr: 2e-05 wd: 0.0005 d_model: 128 n_heads: 16 n_layers: 4 d_ff: 1024 batch_size: 1024 dropout: 0.33542400077351175 margin: 0.899750439570496 epsilon: 0.0001 Train losses: 0.871255449950695 0.8774424940347672 0.8477723076939583 0.8432726114988327 0.8385357707738876 0.8285306915640831 0.8434470370411873 0.8146820440888405 0.8311741352081299 0.8340523391962051 0.8292890638113022 0.8177940174937248 0.8134961426258087 0.8111459836363792 0.8134596198797226 0.7965389341115952 0.815863199532032 0.8064175695180893 0.8025645464658737 0.8023230880498886 0.8013557642698288 0.8014055863022804 0.8032416850328445 0.8020815327763557 0.8051280006766319 Val losses: 0.401462197303772 0.3678644299507141 0.36041259765625 0.36825045943260193 0.40202873945236206 0.36747273802757263 0.41154658794403076 0.43727901577949524 0.42180415987968445 0.3701034188270569 0.3977157175540924 0.3819514214992523 0.3795875608921051 0.3907860517501831 0.3827023506164551 0.3843300938606262 0.39919331669807434 0.38927000761032104 0.3752993941307068 0.3975062370300293 0.37851500511169434 0.37137311697006226 0.3696715831756592 0.38930636644363403 0.3731990158557892 Testloss: 0.5020103540889383 
