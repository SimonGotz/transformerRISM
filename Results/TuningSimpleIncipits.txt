Run 0, config: Learning rate: 0.00E+00 margin: 0.81 batch_size: 128 number of layers: 1 Train losses: 0.6560974445805621 0.5604091580234357 0.5162836104186613 0.5397983144468336 0.5134366692891762 0.5007936340659412 0.4623487275927814 0.4582050976468556 0.44831443856011577 0.42976757928506654 0.4153465023681299 0.40110764129837945 0.36867166988885225 0.34707513896387016 0.3391106909335549 0.3332027276950096 0.3248670087821448 0.32439206740749416 0.30923214132216437 0.2908492135023003 0.28776911256918264 0.28499329890777814 0.2824911724275617 0.27805542945861816 0.2728384441849011 Val losses: 0.6254028252192906 0.6514768174716404 0.6429364000047956 0.6089068949222565 0.7389531305858067 0.6914204699652535 0.6897940465382167 0.6284837829215186 0.6560262101037162 0.5995032063552311 0.6440738822732653 0.6286424249410629 0.6120693747486387 0.5688489867108208 0.5573717185429165 0.5971330787454333 0.5464876123837062 0.5775452426501683 0.5483142541987556 0.5659709764378411 0.5256179975611823 0.5340706727334431 0.569601331438337 0.5401118163551603 0.5404558905533382 Testloss: 0.5897026183088685 
Run 1, config: Learning rate: 0.00E+00 margin: 0.32 batch_size: 32 number of layers: 3 Train losses: 0.2276808105133198 0.25602378618937954 0.36582479808065627 0.416139705810282 0.3757113320408044 0.36557373564552376 0.34793819266336934 0.34139542563094033 0.33292864859104154 0.3261494596799215 0.3208969247010019 0.30481670410544787 0.30258314079708526 0.29782049666952204 0.28927084019890537 0.2795017182826996 0.27172186865850734 0.26861808264145143 0.2670813632232171 0.2641616588389432 0.25533641499501686 0.2604037368739093 0.25391586922385073 0.24853065273276082 0.2512293145060539 Val losses: 0.5913221543295342 0.6479196083127406 0.9757893650155318 0.9932994016429835 0.9972460290841889 0.9910238615253515 1.0215103793562503 0.9810380486019871 0.972849496623926 0.9810508771946556 1.0001036503858733 0.8739811527101617 0.8940139381509078 0.891361141413973 0.8665549619156018 0.8332039149183976 0.8440573131828978 0.8679459722418534 0.7859299972391965 0.7916064204876883 0.8184896958501715 0.8078977622483906 0.7675066234772665 0.789649930962345 0.7723254435940793 Testloss: 0.6200755475856754 
Run 2, config: Learning rate: 0.00E+00 margin: 0.39 batch_size: 16 number of layers: 1 Train losses: 0.43487731107959043 0.43095960252814824 0.42428294795530813 0.4274104321444476 0.40959089475649374 0.41602669656276703 0.4034270387004923 0.38997787568304276 0.3952388675124557 0.40465196227585826 0.39310738024888214 0.39679688855453776 0.40259273604110435 0.3775142189529207 0.3840125521024068 0.3813038925329844 0.3800438164560883 0.3948577267152292 0.3904116815990872 0.3804437896719685 0.3792365231999644 0.3787726178213402 0.3739649774851622 0.36881867856891065 0.3855423416252489 Val losses: 0.7772023620812789 0.7723740981972735 0.7528764636620231 0.7636021702185921 0.7580403141353441 0.7509016918099445 0.754263420726942 0.7265872893126114 0.7373749157656794 0.7396480259688004 0.7106071663939435 0.7267642777899037 0.742985739396966 0.7167364410732103 0.7294718778651693 0.7340121538742729 0.713649000292239 0.7257568390473076 0.7190564622049747 0.7283719182014465 0.6760449818942857 0.7287699792696082 0.733052997485451 0.7261215220326963 0.7210438847541809 Testloss: 0.67779482331938 
Run 3, config: Learning rate: 0.00E+00 margin: 0.59 batch_size: 16 number of layers: 2 Train losses: 0.5158991309227767 0.48685026985627633 0.4507211838607435 0.4147808272529531 0.3566474441024992 0.3271544778788531 0.30516462248784526 0.2821688505234542 0.25517397191789415 0.24293199446466235 0.2209977681990023 0.2151049945089552 0.21066485234984644 0.19045164463696657 0.18576051968115348 0.1839979682807569 0.18291765903985058 0.1738495268203594 0.16733315763650117 0.17012784900488676 0.1738462198663641 0.16344108934755677 0.1618244641356998 0.1624626006241198 0.16503308525791874 Val losses: 0.6766288446343464 0.6057579724685006 0.578715295376985 0.5917053911996925 0.49947575771290326 0.534677313980849 0.5068999578123508 0.5223227433536364 0.4704297029453775 0.4832353993602421 0.47835185009500253 0.4794934570789337 0.44520266729852426 0.4864269285098366 0.4634920957295791 0.4371990118337714 0.4417353192101354 0.39225927176682845 0.44603005725404493 0.42643934695617014 0.4078909526700559 0.35252947392671 0.4036772575067437 0.41533877331277597 0.41588788162107054 Testloss: 0.4595378677887507 
Run 4, config: Learning rate: 4.06E-06 margin: 0.98 batch_size: 16 number of layers: 1 Train losses: 0.7836461351977454 0.7860877460903591 0.747736617481267 0.7233473504031146 0.7147124697764714 0.7008420021445663 0.6926499089709035 0.6716665022903019 0.6643270475996865 0.6505801899565591 0.6535906698968675 0.6285206961411017 0.6190919084681406 Val losses: 0.7241362457690032 0.6622948605081309 0.7049434786257537 0.6581907697345899 0.6483513365621152 0.6394047742304595 0.6482312959173452 0.6402801674345265 0.6231173847032629 0.5649028710697008 0.5839267367902009 0.5891998405041902 0.5898894087128017 Testloss: 0.6262574863073183 
Run 5, config: Learning rate: 0.00E+00 margin: 0.27 batch_size: 32 number of layers: 3 Train losses: 0.3436348216401206 0.3098849140935474 0.2661963832599145 0.21691439659507186 0.18913114766279857 0.16154993330990827 0.15129531307352914 0.13658389686434358 0.12566690505654723 0.1290735325327626 0.12413876608566002 0.11627873480319977 0.11047477683535328 0.1005070576513255 0.09531455150356999 0.09637873603238 0.09471371267680768 0.09081935258927169 0.08351986860787428 0.08395356662847378 0.08834669843867973 0.0834767708071956 0.08569748429236589 0.08081204725636376 0.07935603465195055 Val losses: 0.6476770414595019 0.6252108739133466 0.6184631455362889 0.6161234718665742 0.5802455507872397 0.5568916473472327 0.5613825979985689 0.5232333389290592 0.5224571102543881 0.494153978008973 0.493889544093818 0.485514795048195 0.4805413662341603 0.48642111556571827 0.4467429628497676 0.505235867542133 0.45434553921222687 0.47083718264312074 0.44436013149587733 0.4709526279516387 0.47063918647013214 0.4576458115326731 0.44733469763345884 0.4611193307659082 0.4907984320531812 Testloss: 0.41655892053209465 
Run 6, config: Learning rate: 3.69E-07 margin: 0.88 batch_size: 16 number of layers: 3 Train losses: 0.7054745234824993 0.6876081935785435 0.6319996539089415 0.5893618713926386 0.5303740561008453 0.4702755723838453 0.4352906462219026 0.4037944080653014 0.39236537295359153 0.35356878748646486 0.3450457000070148 0.3290526693617856 0.3195852099745362 0.29526624502959076 0.29338884397789283 0.2856403355245237 0.27500819442448793 0.2784406447852099 0.27468064383224206 0.27107091203883843 0.2577139205402798 0.26409796917880024 0.24538956152068245 Val losses: 0.6849620824274809 0.6546199643093606 0.6277354287064594 0.5817659813424815 0.55819302589997 0.5023277567780536 0.4807726061862448 0.47563051710958065 0.5137832175130429 0.45715395160343336 0.4832231666730798 0.4736965143162271 0.47079010709472324 0.4637579098991726 0.4435614233431609 0.49125655630360476 0.436776588274085 0.4391172494577325 0.47285585610762887 0.4449034193287725 0.4734204033146734 0.4798025670258895 0.4933769350466521 Testloss: 0.460244448232625 
Run 7, config: Learning rate: 0.00E+00 margin: 0.27 batch_size: 128 number of layers: 3 Train losses: 0.3160342798749013 0.21213955999310338 0.15564081261851895 0.12488549908817705 0.11111252443559134 0.08857411502012566 0.08205793347598901 0.07669752096729492 0.065112847112008 0.05871919876159127 0.057810439714300096 0.049799854519651895 0.04418930394658402 0.045530459106858094 0.043327263820527206 0.0368732117227654 0.03704269135843462 0.03373408067359853 0.034867699410933164 0.03276092775944454 0.031322628259658813 0.029297051867887156 0.028411345917787125 0.02860314165478322 0.028197627403398057 Val losses: 0.6211118612970624 0.5335474844489779 0.44728016001837595 0.4542535926614489 0.4151456079312733 0.4124626325709479 0.4107759850365775 0.431414001754352 0.3929173286472048 0.40405241932187763 0.37539765877383097 0.3369696374450411 0.38215239558901104 0.3572002521583012 0.34512059709855486 0.35860370099544525 0.3569369443825313 0.37620762629168375 0.3432958424091339 0.3249181200351034 0.34358045033046175 0.34846682633672443 0.33311587146350313 0.3341651920761381 0.34968799352645874 Testloss: 0.38308212578071255 
Run 8, config: Learning rate: 0.00E+00 margin: 0.65 batch_size: 64 number of layers: 1 Train losses: 0.5916582094298468 0.6033219778979266 0.5934881585615652 0.5973026169670953 0.5824988967842526 0.5936531729168362 0.5832757086665542 0.5908118718200259 0.5643601874510448 0.5683090214376096 0.5820434742503696 0.5862047888614513 0.5741596696553407 0.5753182318475512 0.5827801863352458 0.5583255233588043 0.5612716683635005 0.5684603909651439 0.560997293834333 0.5677946631555204 0.5729015471758666 0.5633119719999807 0.562130351419802 0.5530892780533543 0.5532212886545393 Val losses: 0.6856414207390377 0.7009998602526528 0.7028944098523685 0.6967542512076241 0.6930475948112351 0.6815966069698334 0.6834166177681514 0.655282084430967 0.6641497420413154 0.639001513166087 0.6225743549210685 0.670699553830283 0.6312517204454967 0.6570870716656957 0.6518121127571378 0.6863013708165714 0.6495489797421864 0.6692229541284698 0.6583226919174194 0.6463777720928192 0.664893239736557 0.6546711847186089 0.6382423064538411 0.6233481860586575 0.6540564298629761 Testloss: 0.6668044015048711 
Run 9, config: Learning rate: 0.00E+00 margin: 0.18 batch_size: 1024 number of layers: 2 Train losses: 0.31176771968603134 0.302922748029232 0.2790822423994541 0.22608519345521927 0.19163980893790722 0.1784898042678833 0.16116395592689514 0.15380661562085152 0.14268158376216888 0.1338229663670063 0.12790297344326973 0.11861916072666645 0.11851906776428223 0.11406694445759058 0.11393826827406883 0.11199105251580477 0.10559679288417101 0.10307732690125704 0.10019055940210819 0.09724466037005186 0.10116834379732609 0.09973054472357035 0.10113988816738129 0.0952145392075181 0.09779241308569908 Val losses: 0.7218729853630066 0.7355906367301941 0.6650698781013489 0.6793254017829895 0.6493596434593201 0.6059991121292114 0.6320293545722961 0.6103205680847168 0.595054030418396 0.5599199533462524 0.552322506904602 0.5788689851760864 0.568014919757843 0.5300222635269165 0.5290695428848267 0.5263643860816956 0.5112479329109192 0.544735312461853 0.5529886484146118 0.4893397390842438 0.5024488568305969 0.5143346786499023 0.5006871819496155 0.5020453333854675 0.48617398738861084 Testloss: 0.4989026073247661 
Run 10, config: Learning rate: 5.17E-05 margin: 0.81 batch_size: 16 number of layers: 1 Train losses: 0.6352961018129631 0.5669223070144653 0.5406793689286268 0.5550734774933921 0.5078733023669985 0.45342669994742785 0.3981484937447089 0.39473820251447184 0.37754713462458717 0.35854970459584834 0.34907720420095656 0.33200052932456686 0.32511047301469026 0.34394007060262893 0.30612570257098587 0.29285864531993866 0.2757693922078168 Val losses: 0.680197026418603 0.6169082278790681 0.6215390386788742 0.721762137827666 0.6865097665268441 0.6518362750177799 0.635557616793591 0.6138596444026284 0.6044055083523626 0.5870490457700647 0.5998429764872012 0.5384099843709366 0.5971653259318808 0.5438533588596012 0.5522757745307425 0.5595832905043726 0.569087815284729 Testloss: 0.5619267922746111 
Run 11, config: Learning rate: 2.58E-07 margin: 0.81 batch_size: 32 number of layers: 3 Train losses: 0.6166716003859485 0.6181993908352322 0.6150901563741543 0.6225860257943471 0.6118744820356369 0.599611950914065 0.5964694879673146 0.6142686588896645 0.6229566762844722 0.5918429164974778 0.5935037534545969 0.5880218547803384 0.6032503099353225 0.5830843145096744 0.585226571559906 0.583820789831656 0.5893806211374424 Val losses: 0.5921086229776081 0.6351665467546698 0.6117758013700184 0.6755713736801817 0.629204942991859 0.6433187163712686 0.6343355884677485 0.6325863552720923 0.6559561611267558 0.651336856055678 0.6432580602796454 0.639765341030924 0.6388965416372868 0.5974346995353699 0.6003254191917285 0.6174235631499374 0.6249716072751764 Testloss: 0.6701303803553651 
Run 12, config: Learning rate: 0.00E+00 margin: 0.91 batch_size: 512 number of layers: 1 Train losses: 0.7934709414839745 0.7888893187046051 0.7908765189349651 0.7762018330395222 0.7905593886971474 0.7850457690656185 0.7791095264256 0.787650290876627 0.7851678729057312 0.788958266377449 0.7748752273619175 0.775271050632 0.7750793844461441 0.7742527797818184 0.7777690887451172 0.7682479247450829 0.7630013599991798 0.7655194997787476 0.7865296229720116 0.7810829617083073 0.7598946020007133 0.7819309532642365 0.7534924894571304 0.7913710698485374 0.7776537910103798 Val losses: 0.7065803408622742 0.7079155445098877 0.705793559551239 0.7422100702921549 0.7151180704434713 0.6621091763178507 0.677742580572764 0.7425968050956726 0.6807948152224222 0.6951797008514404 0.72697780529658 0.7242415150006613 0.690956711769104 0.7197331587473551 0.6977742711702982 0.7012148896853129 0.6970200935999552 0.7576093475023905 0.7301478584607443 0.713114062945048 0.6761024395624796 0.7101821899414062 0.7426325480143229 0.6957941253980001 0.7032827734947205 Testloss: 0.7296936752859288 
Run 13, config: Learning rate: 0.00E+00 margin: 0.11 batch_size: 64 number of layers: 3 Train losses: 0.24602955436265028 0.135459801113164 0.09969820490589848 0.08906707537394982 0.07675890928065335 0.0680023396456683 0.06146196005520997 0.054987054290594875 0.05107028362927613 0.04629656865640923 0.03954894459909863 0.036930462938767894 0.035383900503317516 0.03431407196654214 0.032843097382121614 0.03041419287522634 0.02584514915943146 0.026161812466603738 0.02405015500607314 0.02327619332958151 0.02279276803687767 0.02097478801453555 0.021657855919113865 0.019197292184388197 0.020034716416288307 Val losses: 0.6247225105762482 0.576916825558458 0.48187606781721115 0.44914923927613665 0.43578697741031647 0.4504514677183969 0.42435046072517124 0.4142273451600756 0.41812949095453533 0.422352597117424 0.39805655660373823 0.40202305572373526 0.3761650728327887 0.37028075009584427 0.35398981400898527 0.3355630487203598 0.3661166823336056 0.3606184055762632 0.3556645118764469 0.36158761967505726 0.3335775647844587 0.35277662851980757 0.34222565005932537 0.35413854143449236 0.3347101685191904 Testloss: 0.3856323245794048 
Run 14, config: Learning rate: 0.00E+00 margin: 0.92 batch_size: 128 number of layers: 1 Train losses: 0.783098745701918 0.7600570054196599 0.7618851723955639 0.7548692119655325 0.7519068691267896 0.7303633334031746 0.7317029033134232 0.7300622383160378 0.7158684463643316 0.7098995055725326 0.7144434389783375 0.7058108315539005 0.7020126874767133 0.6896808378732027 0.6972135812488954 0.693055991806201 0.6907406262497404 0.681732573615971 0.6924465280860218 0.6930597566846591 0.6834335665204632 0.6798913287582682 0.6710664256295161 0.6785236276797394 0.6774509842716047 Val losses: 0.6915384062698909 0.701400854757854 0.6951088522161756 0.6897514717919486 0.7089654292379107 0.6928729116916656 0.7211575933865139 0.6725554934569767 0.6987560221127102 0.7029911066804614 0.6746343799999782 0.6657700538635254 0.6864473606858935 0.6981220543384552 0.6446662630353656 0.6731312317507607 0.622602790594101 0.6613391297204154 0.6288761411394391 0.6460766920021602 0.6695787949221474 0.6592698224953243 0.6417558746678489 0.6452597635132926 0.6729797167437417 Testloss: 0.690376824781355 
Run 15, config: Learning rate: 0.00E+00 margin: 0.67 batch_size: 32 number of layers: 1 Train losses: 0.5071800642543369 0.5304639968607161 0.556785073324486 0.556735836079827 0.49901371040829906 0.43543522694596537 0.39601887817736026 0.3746319897748806 0.3386903504530589 0.32872280936550213 0.30770063306446427 0.30403418629257767 0.28833359529574715 0.26740527268913056 0.2626937176342364 0.25921075410313077 0.24433324419789845 0.24089436762862734 0.22720508636147888 0.21409819253064968 0.22184854790016456 0.20037670957821388 0.21583721367297348 0.21142529723820863 0.21147262597525562 Val losses: 0.6939022373734859 0.7888305265652505 0.8668977911012214 0.825331567672261 0.8366391418272989 0.6991561881282873 0.6406896977048171 0.7032514632793895 0.6634474705185807 0.5864306123633134 0.6073871986907825 0.5706177806122261 0.5853807745795501 0.6272274313265818 0.5935485655801338 0.5959719691360206 0.5976202754597915 0.5826385737511149 0.5964940906616679 0.5734358278282902 0.5639565932123285 0.5594774536919176 0.5455194371834136 0.5607836523599792 0.5140870142924158 Testloss: 0.576617456654611 
Run 16, config: Learning rate: 0.00E+00 margin: 0.3 batch_size: 128 number of layers: 1 Train losses: 0.39876394218473293 0.3310565837283633 0.28932881132880256 0.25794732570648193 0.23636638362016252 0.22054931610377868 0.20887127896743035 0.20528202692964184 0.18714503988401213 0.18678954243659973 0.18349583869549765 0.17725518034465276 0.1666416112166732 0.1605968738892185 0.16885215855801283 0.1576014485821795 0.1553652981546388 0.15379917254643655 0.1578069002325855 0.1485898029893192 0.14911598834528852 0.15039540304621654 0.14848849488728083 0.1447101981559796 0.14488200587567998 Val losses: 0.7147782019206456 0.6547940288271222 0.5810283997229168 0.6384787474359784 0.619045091526849 0.5933768025466374 0.5637086076395852 0.551422153200422 0.5844348413603646 0.5204498427254813 0.5461969460759845 0.5334093017237527 0.47693384332316263 0.5427536985703877 0.48805772406714304 0.5105862979378019 0.47247073905808584 0.4862718901463917 0.5370892265013286 0.5090712223734174 0.47284907954079763 0.5079543547970908 0.503277484859739 0.49214219408375875 0.49963473209312986 Testloss: 0.5151343242597863 
Run 17, config: Learning rate: 2.95E-04 margin: 0.49 batch_size: 64 number of layers: 3 Train losses: 0.2841048687696457 0.2757000423139996 0.3333986985462683 0.44338882918711064 0.49696483501681576 0.5444508751233419 0.5158270257490652 0.5241453742539441 0.514414518630063 0.50906857141742 0.5067411096007736 0.5054187931396343 0.4998836241386555 0.4991898512398755 0.490707426821744 0.4939555397740117 0.4952064127833755 0.49551369349161783 0.49045156902737086 0.491036550866233 Val losses: 0.5224250789199557 0.5806625708937645 0.7238486515624183 0.7705043447869164 1.022507050207683 0.9478470236063004 0.9849717084850583 1.0004182904958725 0.9992601530892509 1.001786200063569 0.9925369152000972 1.005913251212665 0.9770666318280357 0.9590935621942792 1.0609122472149985 1.044559964111873 1.0003933225359236 1.009717475090708 1.0469118420566832 1.0915929887975966 Testloss: 0.5702571722058462 
Run 18, config: Learning rate: 0.00E+00 margin: 0.83 batch_size: 1024 number of layers: 2 Train losses: 0.6652801483869553 0.6721246466040611 0.6672620624303818 0.654623493552208 0.6457553505897522 0.6603094786405563 0.6560669243335724 0.6429185420274734 0.6388573795557022 0.6279082223773003 0.6488242149353027 0.6279678046703339 0.6312614008784294 0.6411639302968979 0.620943546295166 0.6312003210186958 0.6202041432261467 0.622772328555584 0.6185212284326553 0.6215010359883308 0.6148229688405991 0.6158554926514626 0.6114488616585732 0.6227817237377167 0.6153039857745171 Val losses: 0.670900821685791 0.6385849118232727 0.6834391355514526 0.6370560526847839 0.6621213555335999 0.6677862405776978 0.6661930084228516 0.6009883284568787 0.6098170280456543 0.6758221387863159 0.6156988143920898 0.6179007291793823 0.6745419502258301 0.6585475206375122 0.6326926946640015 0.6141069531440735 0.6206074953079224 0.6174696087837219 0.6276301145553589 0.674702525138855 0.6146838665008545 0.6720085144042969 0.6279267072677612 0.6060937643051147 0.6324977874755859 Testloss: 0.6567696236971583 
Run 19, config: Learning rate: 3.69E-06 margin: 0.34 batch_size: 16 number of layers: 1 Train losses: 0.41246336334281497 0.4040058739759304 0.3843815785867197 0.3711400989029143 0.3548929959535599 0.34742578897211285 0.333303580460725 0.32223128687452385 0.31147145882800775 0.3033260855409834 0.2948933889468511 0.2926469801752656 0.2886311888694763 0.2890212798560107 Val losses: 0.7482382629228674 0.7493267178535461 0.7196419047272724 0.7335672093474347 0.6869658695614856 0.7250753096912218 0.6815541285535563 0.6720169951086459 0.7017177830571714 0.6839466271193131 0.6487981467143349 0.6922265970188638 0.7000074884165888 0.7085682526878688 Testloss: 0.6741994497648515 
Run 20, config: Learning rate: 0.00E+00 margin: 0.24 batch_size: 16 number of layers: 3 Train losses: 0.19278182199707738 0.20779197834156177 0.29563529094060265 0.3547706098468215 0.3222696849041515 0.29527639553502755 0.2824380158826157 0.2658087577532839 0.2507003684838613 0.2379950632099752 0.2263128916974421 0.21881193115755362 0.21178788802138082 0.206740143812365 0.20018803849816322 0.19939377995000945 0.18438742927930973 0.1876045424905088 0.18248961766560873 0.18557439482322446 0.1795940252089942 0.17411873075697157 0.17091177155574164 0.17361624124977323 0.1717405672426577 Val losses: 0.5025413948556651 0.6228826818258866 0.7905291234669478 0.9858446167862933 0.9895275131515835 0.9006878046885781 0.9393832134163898 0.8553737946178602 0.8332742325637652 0.7682159286478292 0.815374583783357 0.7298095397327258 0.7335290281668954 0.7301321744918823 0.6986688463584236 0.6668280207592509 0.7241205785585486 0.7212096219477446 0.6669679786847985 0.6712701825991921 0.6293753416641898 0.6440608742444411 0.6841712632904882 0.6470674360575883 0.6125561826902887 Testloss: 0.5656284192392853 
Run 21, config: Learning rate: 0.00E+00 margin: 0.9 batch_size: 32 number of layers: 3 Train losses: 0.6196169263786739 0.4633299245878502 0.34743540044184085 0.2984054734309514 0.2523804350031747 0.21665466571295702 0.19528623079812085 0.17642349667019314 0.16772748607176322 0.15053042085082444 0.1430549446079466 0.12619912381525392 0.11896129614777035 0.11034010902599052 0.10679831703503927 0.09318856652136202 0.09224125864329162 0.09273933536476558 0.07695527727957126 0.08185966423264256 0.06651037280206328 0.07694240124137314 0.06370732618702782 0.05811578719704239 0.06545952823426988 Val losses: 0.5580308076582456 0.46622703315918906 0.4510157364502288 0.461066322368488 0.438231136715203 0.4199386395906147 0.4673826244839451 0.41275576384443985 0.4324525589482826 0.3823965118642439 0.3670281778302109 0.4453155795733134 0.3673071280906075 0.3749712768353914 0.4017025666278705 0.36684253236703707 0.3795112559669896 0.4119582150066108 0.40782205577482256 0.3647974039378919 0.369777371486028 0.3406102845543309 0.3527889539275253 0.35781673433487876 0.3264881063971603 Testloss: 0.4352332908098663 
Run 22, config: Learning rate: 1.48E-07 margin: 0.39 batch_size: 128 number of layers: 3 Train losses: 0.3955898912095312 0.4026068444572278 0.3997350504149252 0.39058745752519636 0.39366115518470307 0.3897778996780737 0.39692722105268224 0.3811849395730602 0.39430980540033594 0.37716650162170184 0.38585285308645734 0.3760844255561259 0.38133372833479695 0.37372867340472204 0.379442468507966 0.383327297310331 0.390186824015717 0.37387295756767047 0.3804509057037866 0.36730650089569944 Val losses: 0.6195572061198098 0.6412344362054553 0.6576489870037351 0.6357703251498086 0.6343745248658317 0.6444150848048074 0.6203492156096867 0.6039923557213375 0.6387059773717608 0.6293500661849976 0.6143149265221187 0.5953402710812432 0.6183416630540576 0.606366583279201 0.6509403586387634 0.632185989192554 0.6085190687860761 0.6096607404095786 0.6213694981166294 0.6332500960145678 Testloss: 0.6270149922383791 
Run 23, config: Learning rate: 1.18E-06 margin: 0.16 batch_size: 32 number of layers: 2 Train losses: 0.3038100726074643 0.30786241933151526 0.30471192730797664 0.2975866166529832 0.29857888442498665 0.2916896805719093 0.2880074928204219 0.2858941281283343 Val losses: 0.6775607817005693 0.6699719235562441 0.6620948382636958 0.7001269543380068 0.6488344915080488 0.6542009439384728 0.6566002086589211 0.6929202733332651 Testloss: 0.6522790062820377 
Run 24, config: Learning rate: 0.00E+00 margin: 0.82 batch_size: 512 number of layers: 2 Train losses: 0.6578086093068123 0.6402287892997265 0.6550372466444969 0.6482743509113789 0.6363223306834698 0.6616863310337067 0.6398189477622509 0.6352558098733425 0.6529222466051579 0.6437511704862118 0.64525156468153 0.6473591066896915 0.6282747909426689 0.6363065540790558 0.6434070654213428 0.6382305584847927 0.6433733887970448 0.6387705355882645 0.6409696936607361 0.6267848983407021 0.6381014958024025 0.6327798664569855 0.6407084353268147 0.6476311832666397 0.6263535618782043 Val losses: 0.7247591217358907 0.6967220703760783 0.7051131725311279 0.7598927021026611 0.6855476101239523 0.6834693551063538 0.7274469534556071 0.6946505705515543 0.6869799494743347 0.6849714716275533 0.7019137144088745 0.7143216530481974 0.6991902391115824 0.711925188700358 0.6895617643992106 0.7469642957051595 0.6949979265530905 0.7290834387143453 0.7211159666379293 0.744244376818339 0.7132912476857504 0.7082517941792806 0.7263968586921692 0.7168590426445007 0.7200284401575724 Testloss: 0.7489025192219525 
