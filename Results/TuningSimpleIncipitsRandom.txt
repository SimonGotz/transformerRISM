Run 0, config: lr: 1e-05 wd: 0.01 d_model: 16 n_heads: 16 n_layers: 4 d_ff: 2048 batch_size: 16 dropout: 0.6718112039423338 margin: 0.9010049705486168 epsilon: 1e-05 Train losses: 1.0354424123410826 1.0239645014206569 1.0208017140075012 0.9939369919123473 0.9927264775390978 0.9847899607486195 0.9783101749089029 0.9745388519432809 0.9716334820897491 Val losses: 0.6568909567335378 0.672315435824187 0.6438627325970193 0.6505713382492895 0.6916722144769586 0.6777390547420667 0.6950498150742572 0.6968337831289871 0.722727152057316 Testloss: 0.6988976041347127 
Run 1, config: lr: 1e-06 wd: 0.05 d_model: 256 n_heads: 2 n_layers: 1 d_ff: 2048 batch_size: 32 dropout: 0.7168349845867351 margin: 0.36402669955692535 epsilon: 1e-07 Train losses: 0.6540584327997985 0.6503911667399936 0.6672978140689708 0.6498808640020864 0.6702952203927217 0.6594566111211424 0.6537223829163445 0.6561592737833659 0.6703579196223506 0.6728585009221677 0.650034734054848 0.6689352865572329 0.6528353379832373 0.6590351793501112 0.6582330917870557 0.6732686769079279 0.6587044499538562 0.6437775278532947 0.6651035028475302 0.6534238446641851 0.6535833440445088 0.6469362404611375 0.6453524766144929 Val losses: 0.40649196583973735 0.3972200006246567 0.4362121266231202 0.4477719494648147 0.42643602375398604 0.44705279877311305 0.4614858922728321 0.4375840331378736 0.4396068809325235 0.4359548559837174 0.4248025218645732 0.4105741227405113 0.4059461906813739 0.43041490358218815 0.4578320776161395 0.4483259822192945 0.42552334096348077 0.43579103287897614 0.47107275954464023 0.42104180392466095 0.4267015028418156 0.4286840087489078 0.433696348154754 Testloss: 0.4211817102487637 
Run 2, config: lr: 0.0001 wd: 0.001 d_model: 256 n_heads: 4 n_layers: 2 d_ff: 256 batch_size: 64 dropout: 0.5335608682321682 margin: 0.35885381198726424 epsilon: 0.01 Train losses: 0.6652803621910236 0.662827941664943 0.6720860134672235 0.6735433556415417 0.6657325157412777 0.6512030822259408 0.6679944881686458 0.6676501490451672 0.6533459740656393 0.6647313623516647 0.6726875556839838 0.6631425208515591 0.6587657705501274 0.6624867600423319 0.6657465559464913 0.6656036376953125 0.6595359327616515 0.6650945361013766 0.6682663319287476 Val losses: 0.3243296928703785 0.31775512439864023 0.3286255257470267 0.3255591887448515 0.32871315787945476 0.3459815766130175 0.328159861266613 0.3256663350122316 0.3455304971763066 0.3452629681144442 0.3392864337989262 0.34024913502591 0.3492175723825182 0.32571300332035336 0.3403596319258213 0.3374809761132513 0.3423483722976276 0.3438889698258468 0.35519866911428316 Testloss: 0.37582451765244873 
Run 3, config: lr: 2e-06 wd: 0.005 d_model: 16 n_heads: 2 n_layers: 4 d_ff: 2048 batch_size: 32 dropout: 0.1170425249151224 margin: 0.29478331917910705 epsilon: 1e-06 Train losses: 0.49830880143024303 0.47328958572060975 0.41433388198967336 0.3707384576952016 0.3322181901446095 0.3227473345067766 0.31517637730748566 0.30356737518752064 0.3015120487522196 0.3032066583081528 0.30057399377778726 0.3020144835666374 0.30224426526714254 0.29641041590107814 0.2913305259413189 0.2942671039590129 0.29006473852528464 0.29425572013413465 0.29118966311216354 0.29233474792153746 0.2906835639918292 0.29377486805121106 0.2856483752528826 0.29556632185423815 0.29228830917014015 Val losses: 0.32399186413539083 0.320299100980424 0.27194335729929436 0.24356427501168168 0.23246934016545615 0.23378289634721322 0.22106483447970005 0.23516840657644106 0.23562640343841754 0.23195782669803552 0.22974525104489243 0.23190105797951682 0.23276339941903165 0.23102301620600516 0.22371224353187963 0.2344259478543934 0.23256713125789374 0.2370223089268333 0.2264129469792048 0.23044526498568685 0.23654428281282125 0.23343872684135772 0.21611575756156653 0.23172220901439064 0.23658176498454914 Testloss: 0.23086785890166017 
Run 4, config: lr: 0.0001 wd: 0.01 d_model: 256 n_heads: 8 n_layers: 4 d_ff: 128 batch_size: 512 dropout: 0.23134761087313171 margin: 0.9209015328273347 epsilon: 1e-07 Train losses: 0.8514783829450607 0.8524992503225803 0.8411307781934738 0.7962442971765995 0.7747066356241703 0.7748845852911472 0.7194543369114399 0.6772112175822258 0.6461845301091671 0.6277204006910324 0.6105657443404198 0.5737673528492451 0.5548794232308865 0.5498196482658386 0.5491295009851456 0.5336844697594643 0.5171186048537493 0.506425105035305 0.49768806248903275 0.49483065120875835 0.4984063506126404 0.49432668648660183 0.48905036225914955 0.48846036940813065 0.5141812767833471 Val losses: 0.6011373003323873 0.6010162432988485 0.5923243562380472 0.5525899728139242 0.5760297377904257 0.5586061080296835 0.49262499809265137 0.5517494082450867 0.5964516202608744 0.5514769554138184 0.6132763822873434 0.5713526606559753 0.5347981850306193 0.5765923659006754 0.5227389931678772 0.5274803837140402 0.5677255988121033 0.5340044498443604 0.5234505633513132 0.5493979851404825 0.52577805519104 0.5302769045035044 0.5206270615259806 0.5454715092976888 0.578336258729299 Testloss: 0.5272473856154678 
Run 5, config: lr: 0.001 wd: 0.0005 d_model: 32 n_heads: 2 n_layers: 1 d_ff: 512 batch_size: 64 dropout: 0.5943232702544616 margin: 0.3907793933186456 epsilon: 1e-07 Train losses: 0.6761666869675672 0.593310221919307 0.5564963161945343 0.5554533686902788 0.5349660725505264 0.5159417446012851 0.513648576868905 0.5024973827379721 0.4884351432323456 0.4841739590521212 Val losses: 0.3922514553580965 0.3378317185810634 0.33969048517090933 0.33759463897773195 0.31572097327028004 0.3326552041939327 0.32934505279575077 0.3313364971961294 0.3322176603334291 0.33240085414477755 Testloss: 0.3217719167004013 
Run 6, config: lr: 2e-06 wd: 0.05 d_model: 256 n_heads: 2 n_layers: 3 d_ff: 1024 batch_size: 64 dropout: 0.11021661117137516 margin: 0.5585060082556325 epsilon: 1e-07 Train losses: 0.5373544346403193 0.5478214153537044 0.5399082199290947 0.5485361966821882 0.526202216413286 0.5343600632967772 0.5346017047210976 0.5207021057605743 0.539928201172087 0.5212911387284597 0.5101767005743804 0.49773246116108366 0.5133063868240074 0.5188440492859593 0.5109154182451743 0.512313981850942 0.48814290210052774 Val losses: 0.5011497257011277 0.5422889494470188 0.4979898961527007 0.5186972820333072 0.4986614499773298 0.5158577178205762 0.4661911502480507 0.4726328051515988 0.48868176553930553 0.48485774440424784 0.488442953143801 0.4960163193089621 0.4845905953219959 0.4432675412722996 0.4670069430555616 0.4728971613304956 0.47790511697530746 Testloss: 0.4310315519587405 
Run 7, config: lr: 0.001 wd: 0.0005 d_model: 16 n_heads: 2 n_layers: 2 d_ff: 256 batch_size: 1024 dropout: 0.7370885722404203 margin: 0.5077928975296363 epsilon: 1e-05 Train losses: 0.7481811568140984 0.7397182434797287 0.7215467393398285 0.6984733566641808 0.6736403703689575 0.6689506694674492 0.648405060172081 0.637008547782898 0.6369927227497101 0.6395431682467461 0.6317289024591446 0.6287676692008972 0.6179011538624763 0.6167967468500137 0.6311445087194443 0.6077163591980934 Val losses: 0.4490887522697449 0.44833511114120483 0.42188212275505066 0.4111211895942688 0.41477376222610474 0.43455514311790466 0.4328438937664032 0.4310220777988434 0.43810373544692993 0.44053930044174194 0.43818265199661255 0.4449106454849243 0.44211292266845703 0.44232919812202454 0.44277864694595337 0.447767049074173 Testloss: 0.41206091721272997 
Run 8, config: lr: 1e-06 wd: 0.0001 d_model: 128 n_heads: 2 n_layers: 1 d_ff: 128 batch_size: 64 dropout: 0.02405126680836647 margin: 0.7633904151056753 epsilon: 1e-05 Train losses: 0.6592322360586237 0.6620807594723171 0.6495218400601988 0.6550558544971324 0.6375351883746959 0.6440480051217256 0.6620445397165087 0.6567389327066916 0.6468465782977917 0.6509520574852272 0.6636440113738731 0.647313713365131 0.6735238635981524 0.660467165046268 0.6625965431884483 0.6477962957488166 0.6573440891725045 0.6465150950131593 0.6447211409056628 0.6692833439067558 0.649045706236804 0.6531840635670556 Val losses: 0.676971227994987 0.6905822189790862 0.6802037869180951 0.6899018713406154 0.6609791006360736 0.6554983385971614 0.6596718983990806 0.6865154440913882 0.6623725976262774 0.6453512949602944 0.675021863409451 0.7090962869780404 0.7056984944002969 0.6762452466147286 0.6710006701094764 0.6982801152127129 0.644300075513976 0.6840341687202454 0.6642101013234684 0.6882464970861163 0.6893836770738874 0.6989635399409703 Testloss: 0.6555771392335897 
Run 9, config: lr: 2e-06 wd: 0.05 d_model: 16 n_heads: 16 n_layers: 3 d_ff: 1024 batch_size: 128 dropout: 0.7166255406752606 margin: 0.8640986136224867 epsilon: 1e-05 Train losses: 1.0160791349055163 1.0284095682314973 1.0017349995783906 0.9984082713055966 0.9975296685944742 1.007167439852188 1.0065670360380143 1.005172564912198 0.9992609299830536 0.9952238344434482 0.9827101657639689 0.9935857226599508 0.9984778361534005 0.992922654792444 0.9969778728129258 0.997084895176674 0.9994427801957771 Val losses: 0.7512998580932617 0.741811718259539 0.7209040735449109 0.7244023893560682 0.7251739501953125 0.7237961888313293 0.7266370866979871 0.7167311310768127 0.7149934470653534 0.7400064510958535 0.732677664075579 0.7267451243741172 0.7439355509621757 0.6952693377222333 0.7189607577664512 0.7355060832841056 0.7610151043960026 Testloss: 0.7290853768152394 
Run 10, config: lr: 0.0001 wd: 0.005 d_model: 32 n_heads: 8 n_layers: 2 d_ff: 128 batch_size: 512 dropout: 0.4643147084932312 margin: 0.38982321500859457 epsilon: 0.0001 Train losses: 0.7160238549113274 0.6985413767397404 0.7060032971203327 0.6724975071847439 0.6608679965138435 0.6536510810256004 0.6481379754841328 0.6264137551188469 0.6299434415996075 0.6221919916570187 0.6170951388776302 0.6155961006879807 0.6191527806222439 0.5936207063496113 0.5960369035601616 0.5890003331005573 0.6037412397563457 0.5823100507259369 0.5866209156811237 0.5714059062302113 0.5913860686123371 0.5777838490903378 0.5980040542781353 0.5833025425672531 0.5803791843354702 Val losses: 0.3636131485303243 0.3666069209575653 0.3491814037164052 0.3423567016919454 0.3652845621109009 0.3408517340819041 0.3464413285255432 0.3494669596354167 0.3271928628285726 0.3330642779668172 0.31778766711552936 0.3196888168652852 0.33523372809092206 0.31316863497098285 0.319853941599528 0.325138399998347 0.312237153450648 0.34133216738700867 0.3319360415140788 0.32246270775794983 0.3367210825284322 0.3347414235273997 0.3142566482226054 0.33008326093355816 0.3195533553759257 Testloss: 0.3525516474201897 
Run 11, config: lr: 0.0001 wd: 0.1 d_model: 512 n_heads: 16 n_layers: 1 d_ff: 1024 batch_size: 256 dropout: 0.1925846848880359 margin: 0.8604567952274467 epsilon: 0.1 Train losses: 0.8238791227340698 0.7986699198231553 0.8089221228252758 0.8074575590364861 0.8144072709661542 0.8050737164237283 0.797822527813189 0.7938668366634485 0.7941267490386963 0.7951292088537505 0.8001962484735431 0.7937975742600181 0.7985530495643616 0.8025949868288907 0.7979973500425165 0.7920512864083955 0.803500114065228 0.8118608756498857 0.8184657855467363 0.8123396544745474 0.7866647369933851 0.80728582721768 0.8064271247748173 0.7910724354512764 0.8016396074584036 Val losses: 0.6253259352275303 0.5864402779511043 0.6088403633662632 0.5947784696306501 0.5698583211217608 0.5871536902018956 0.6194209200995309 0.6075588975633893 0.5934308171272278 0.6145148362432208 0.5899500676563808 0.6154416714395795 0.630629667213985 0.5931079387664795 0.615134699004037 0.6020370721817017 0.6194628306797573 0.5905939255441938 0.5797365818704877 0.6192858474595206 0.5913601006780352 0.5993309446743557 0.6111137866973877 0.5902227418763297 0.580335795879364 Testloss: 0.6042976554569717 
Run 12, config: lr: 1e-05 wd: 0.0001 d_model: 32 n_heads: 16 n_layers: 1 d_ff: 256 batch_size: 1024 dropout: 0.32664538041332264 margin: 0.7067889096433118 epsilon: 0.001 Train losses: 0.8941161632537842 0.8853199928998947 0.8800643607974052 0.8752279654145241 0.8670502603054047 0.8559353947639465 0.8799601569771767 0.8946579471230507 0.8632159754633904 0.8663660883903503 0.8823413327336311 0.8614088147878647 0.8816305473446846 0.8583179414272308 0.8536374494433403 0.8739798814058304 0.8808034211397171 0.8666845709085464 0.880065955221653 0.8744129985570908 0.8778105825185776 0.8617237508296967 0.8628831431269646 0.8760509267449379 0.8719983473420143 Val losses: 0.5684069395065308 0.5516708493232727 0.5393052101135254 0.5525702238082886 0.591252863407135 0.5708933472633362 0.5355737209320068 0.5340964794158936 0.5448998212814331 0.5355867743492126 0.53630530834198 0.5702856183052063 0.5302748084068298 0.5746131539344788 0.566084623336792 0.558418869972229 0.5495452880859375 0.49659106135368347 0.5668963193893433 0.5678293704986572 0.5531656742095947 0.5963469743728638 0.5568662285804749 0.5356133580207825 0.5074899792671204 Testloss: 0.5481687619968211 
Run 13, config: lr: 2e-05 wd: 0.0005 d_model: 32 n_heads: 4 n_layers: 3 d_ff: 512 batch_size: 1024 dropout: 0.4189024066193214 margin: 0.6311334468090597 epsilon: 1e-05 Train losses: 0.822880208492279 0.8129664286971092 0.8145558759570122 0.8160212188959122 0.8061205521225929 0.8034095615148544 0.7909236401319504 0.794673964381218 0.7885063886642456 0.7691676914691925 0.7846205681562424 0.7773063257336617 0.7698493152856827 0.7738749384880066 0.7703473046422005 0.770388126373291 0.7749509066343307 0.7583318278193474 0.7548811063170433 0.7737666144967079 0.7548190057277679 0.7540970668196678 0.7618831172585487 0.7503902986645699 0.7471482083201408 Val losses: 0.5271311402320862 0.5299593210220337 0.5504388213157654 0.5198566317558289 0.4920702278614044 0.5251620411872864 0.5060564279556274 0.511508584022522 0.5026888847351074 0.524925947189331 0.4968986511230469 0.4888759255409241 0.49474239349365234 0.5197292566299438 0.4921523928642273 0.49406248331069946 0.5066322088241577 0.4946134388446808 0.4889048933982849 0.5034874081611633 0.4986332058906555 0.5083277225494385 0.5031799077987671 0.46460461616516113 0.5010647177696228 Testloss: 0.508231248616013 
Run 14, config: lr: 2e-06 wd: 0.1 d_model: 1024 n_heads: 4 n_layers: 2 d_ff: 1024 batch_size: 16 dropout: 0.018752679460003654 margin: 0.32411045675241346 epsilon: 0.1 Train losses: 0.4364455191073594 0.44588302429075594 0.4327127629960025 0.44488480974126743 0.4405743685033586 0.44528584270565597 0.4437648168316594 0.4376088692082299 0.44442434189496216 0.4468898312913047 0.44555725322829354 0.4537911191030785 0.45044234306723985 0.4429622358745999 0.448883169999829 0.4465855081876119 0.4370611686397482 0.4435283361761658 0.4437352847169947 0.44242485971362505 0.4358759346935484 0.4366181053497173 0.4465914718530796 0.43944993394392506 0.441515702340338 Val losses: 0.48928093547406404 0.5273721959279931 0.5188750712767891 0.4857042794642241 0.508079935675082 0.5367498003918192 0.4931536529375159 0.5060159449991972 0.531417079075523 0.5172361228777015 0.5532564831816632 0.5364701701247174 0.4740866925405419 0.5213418970937315 0.47929180653198905 0.5046764570733775 0.49954379848811936 0.5163524099018263 0.505406085304592 0.4903459756270699 0.5267261862754822 0.5269471883773804 0.5068649074305659 0.49173615237940915 0.5108001683069312 Testloss: 0.46362711660672623 
Run 15, config: lr: 1e-05 wd: 0.01 d_model: 32 n_heads: 16 n_layers: 4 d_ff: 512 batch_size: 1024 dropout: 0.368645454001985 margin: 0.2309613011731985 epsilon: 1e-05 Train losses: 0.5537017956376076 0.5691070035099983 0.5436132475733757 0.5412324443459511 0.5214089043438435 0.5189605057239532 0.5296080708503723 0.5266982987523079 0.5187786631286144 0.5174396336078644 0.5240682512521744 0.505405355244875 Val losses: 0.3377271294593811 0.3561962842941284 0.3117844760417938 0.340687096118927 0.33010178804397583 0.3528541028499603 0.3406498432159424 0.3212704062461853 0.30330151319503784 0.3037928342819214 0.3206234276294708 0.3256896138191223 Testloss: 0.28925432590843214 
Run 16, config: lr: 2e-06 wd: 0.0001 d_model: 256 n_heads: 4 n_layers: 2 d_ff: 2048 batch_size: 256 dropout: 0.7973957156019637 margin: 0.8692537190250247 epsilon: 1e-06 Train losses: 1.0235618298703975 1.0050166827259641 1.007278101010756 0.9877027507984277 1.0130089069857742 1.0129572821385933 1.0065714156988896 1.0092433221412427 1.0087492989771294 1.0041828047145496 0.9969247380892435 1.013398629246336 1.0001792040738193 1.0053053696950276 0.993263921954415 0.98671401088888 0.9782930757060195 0.9978358474644747 Val losses: 0.6346999321665082 0.6190160172326225 0.5990979330880302 0.6079078912734985 0.6311664836747306 0.6132745827947345 0.5935334733554295 0.6192128913743156 0.5889317563601902 0.6100107772009713 0.5876983404159546 0.5885956117085048 0.5756783655711583 0.617994657584599 0.5846501844269889 0.6157997420855931 0.632075812135424 0.6394942487989154 Testloss: 0.6456256386784204 
Run 17, config: lr: 0.002 wd: 0.1 d_model: 128 n_heads: 16 n_layers: 3 d_ff: 2048 batch_size: 256 dropout: 0.11154178473174162 margin: 0.3685745909312769 epsilon: 1e-05 Train losses: 0.34015190240108606 0.2673079412091862 0.2219502799438708 0.19485817804481043 0.17420854938752722 0.15279064350055926 0.13921782780777325 0.12931588766249744 0.11530188064683568 0.1144632597764333 0.10156406213839848 0.09390568168777408 0.08549793685475986 0.08211915477207213 0.07802272000999162 0.06669070571660995 0.07286273981585648 0.06738464455261375 0.06337575591874844 0.061768054510607864 0.058244417223966484 0.05655419160470818 0.051416677168824455 0.050015046120141494 0.0523530107668855 Val losses: 0.24400261683123453 0.24523113667964935 0.25548309300627026 0.32065186330250334 0.26228819148881094 0.24363615896020616 0.25654932643686024 0.22538759452956064 0.22277871625764029 0.22353417319910868 0.2307077114071165 0.21719571948051453 0.22707350764955794 0.1942257583141327 0.18516263152871812 0.18770559344972884 0.18820527621677943 0.17942396657807486 0.17412645263331278 0.17924409891877854 0.16967130984578813 0.18211759201117925 0.19387987681797572 0.18720452061721257 0.18595554786069052 Testloss: 0.16281153780907828 
Run 18, config: lr: 1e-05 wd: 0.005 d_model: 1024 n_heads: 16 n_layers: 3 d_ff: 512 batch_size: 1024 dropout: 0.4066971770636867 margin: 0.5250546610877488 epsilon: 1e-06 Train losses: 0.6512290239334106 0.6528990417718887 0.6468326672911644 0.6626920700073242 0.6597101241350174 0.639878936111927 0.6529576182365417 0.6479430720210075 0.6613414138555527 0.6530646830797195 0.638951800763607 0.6313031390309334 0.6409629285335541 0.6326295658946037 0.6495617255568504 0.6182084158062935 0.6409403085708618 0.6325416564941406 0.6278957203030586 0.6425221487879753 0.6303124502301216 0.643338181078434 0.6392101272940636 0.6267143785953522 Val losses: 0.3907943069934845 0.41492369771003723 0.389123797416687 0.39367765188217163 0.3532898426055908 0.3633743226528168 0.4100215435028076 0.3964754045009613 0.38241368532180786 0.36565566062927246 0.3829779624938965 0.3999324440956116 0.36987560987472534 0.36855000257492065 0.3874369263648987 0.3875143229961395 0.3755667805671692 0.40884488821029663 0.3719036877155304 0.3887050449848175 0.37641000747680664 0.3823310136795044 0.3870830833911896 0.387426495552063 Testloss: 0.41938400526165254 
Run 19, config: lr: 0.001 wd: 0.005 d_model: 256 n_heads: 16 n_layers: 4 d_ff: 1024 batch_size: 128 dropout: 0.037526191889284105 margin: 0.9457259592737651 epsilon: 0.0001 Train losses: 0.6025734890752764 0.4983417182715971 0.36639483806802264 0.28724101853014816 0.22886178642511368 0.20379106244489328 0.17392361030649783 0.1658382225614875 0.14191270397225422 0.12448256907623205 0.11232883438690384 0.09649260742450828 0.09824239195727591 0.08943849679694246 0.08665792899790095 0.0813276027120761 0.06982831134280162 0.06875181687411977 0.06511893646040959 0.05454496049614095 0.05690085387496806 0.0501548653440689 0.0496409143974532 0.05075829109149193 0.05202323821053576 Val losses: 0.6085259573800224 0.49856885841914583 0.45791750720569063 0.45405099434512003 0.47006855905056 0.44380159250327517 0.45305653129305157 0.4697490100349699 0.42233128419944216 0.38284256415707724 0.4527052215167454 0.4767038971185684 0.4123113623687199 0.44545928708144594 0.41955057850905825 0.38359400417123524 0.4009511428219931 0.35014589769499643 0.3853485882282257 0.36257520637341906 0.3831587369952883 0.33425956325871603 0.38580902772290365 0.33003175471510204 0.39774044922419954 Testloss: 0.34235250338034523 
Run 20, config: lr: 0.0002 wd: 0.0001 d_model: 256 n_heads: 16 n_layers: 3 d_ff: 1024 batch_size: 64 dropout: 0.7695212826518897 margin: 0.5319911481098617 epsilon: 0.01 Train losses: 0.7299297240045336 0.7444166929633529 0.7450467617423446 0.7461894256097299 0.7641170501708985 0.7463090521317941 0.7459920384265758 0.7526504300258777 0.7554895690193882 0.7504326785052264 0.7568590899308523 0.7410101449048078 0.7573729610001599 Val losses: 0.3930574059486389 0.39663182305438177 0.3984229766896793 0.4029053369803088 0.37804981055004255 0.4095702682222639 0.4184702028121267 0.387531865388155 0.40550592115947176 0.35611981100269724 0.379077891686133 0.3995285704731941 0.40302612100328716 Testloss: 0.370063944032938 
Run 21, config: lr: 0.001 wd: 0.05 d_model: 32 n_heads: 8 n_layers: 2 d_ff: 512 batch_size: 64 dropout: 0.3469276146882739 margin: 0.3747042873980685 epsilon: 1e-06 Train losses: 0.5525899191697439 0.4684373539906961 0.43847500571498166 0.4259213403419212 0.4146263939362985 0.4027145281985954 0.39039416004110267 0.3731180184417301 0.359115344065207 0.3539798074298435 0.3360519135439837 0.32990036805470785 0.3165789151633227 0.317857849156415 0.3137097662245786 0.29758166604571873 0.29304643792134744 0.2872679552546254 0.28238408234384327 0.28272216695326347 0.28387570756453057 0.28110658990012277 0.2799016226221014 0.276497596612683 0.27134474626293886 Val losses: 0.29261258404169765 0.28206517919898033 0.28621979004570414 0.30640504296336857 0.2847115120717457 0.28031492073621067 0.29364748139466557 0.3016561535852296 0.28384150617889 0.280591680003064 0.28389683099729673 0.2989662289619446 0.2935088745185307 0.2791236617735454 0.2895958700350353 0.3040537206189973 0.28878816057528767 0.28140719128506525 0.26342418576989857 0.24694977487836564 0.26513826474547386 0.2614904668714319 0.27302180816020283 0.25172350502439905 0.27941485441156794 Testloss: 0.2615069040635159 
Run 22, config: lr: 1e-06 wd: 0.005 d_model: 16 n_heads: 16 n_layers: 3 d_ff: 2048 batch_size: 1024 dropout: 0.5200674705287688 margin: 0.8022399666546659 epsilon: 1e-05 Train losses: 0.9452551826834679 0.9378050044178963 0.9378712177276611 0.9666372835636139 0.9577468559145927 0.9527116492390633 0.9583611264824867 0.9387597143650055 0.93865005671978 0.9566979110240936 0.9598781242966652 0.966748021543026 0.9462279677391052 0.939904972910881 0.9686134830117226 0.9537875950336456 0.9411177113652229 0.9475801885128021 0.9590153768658638 0.9533815085887909 0.9359320923686028 0.947224423289299 0.9370550513267517 0.9638758972287178 0.954463467001915 Val losses: 0.6738475561141968 0.6514797806739807 0.6298865675926208 0.6586753726005554 0.643098771572113 0.6462125778198242 0.6296675205230713 0.6097135543823242 0.6339291334152222 0.6051136255264282 0.666887104511261 0.6314125657081604 0.6687090992927551 0.6780937910079956 0.6449403762817383 0.6232333779335022 0.624170184135437 0.6587857604026794 0.6448871493339539 0.6226269006729126 0.6394599676132202 0.6509677171707153 0.634623110294342 0.6768779754638672 0.638144850730896 Testloss: 0.6224994117797612 
Run 23, config: lr: 0.002 wd: 0.005 d_model: 128 n_heads: 16 n_layers: 2 d_ff: 128 batch_size: 16 dropout: 0.34807687038067203 margin: 0.6521555861832367 epsilon: 1e-08 Train losses: 0.7354384228035256 0.6246284429673795 0.5179899348153009 0.49162374138832093 0.47775172618804157 0.4621008296807607 0.45304054043911124 0.42473587090218506 0.40854353717079867 0.41126550998952655 0.4002613483203782 0.38597012487826526 0.36632957916568826 0.37560839357751385 0.3595799496880284 0.33685350183535506 0.3335777384263498 0.3235343196601779 0.3216443523488663 0.3191392466425896 0.31614649199225286 0.3143441650050658 0.29882360487072557 Val losses: 0.44018482060536096 0.5379828041014464 0.5610891230728315 0.5532911615527194 0.5884544543598009 0.49989601632823116 0.5274941051783769 0.5435101733259533 0.5434487360975017 0.46078172688898833 0.451531063085017 0.43829122729923414 0.4562038613402325 0.4208306168732436 0.45361322034960205 0.42641191106775533 0.43285878080388773 0.40086644900881724 0.38303999848987746 0.36571060639360675 0.3927871522696122 0.4180497900299404 0.43765555762726327 Testloss: 0.38194069676886117 
Run 24, config: lr: 1e-06 wd: 0.001 d_model: 256 n_heads: 4 n_layers: 4 d_ff: 128 batch_size: 128 dropout: 0.7879826378317741 margin: 0.14852157305654545 epsilon: 0.01 Train losses: 0.5011111377780118 0.5007129631825348 0.49534141216705097 0.4909542812340295 0.5078980526817378 0.4944811756931134 0.49740172411078837 0.4952885957796182 0.5058801054064908 0.4953651081270246 Val losses: 0.2785754384739058 0.28392616872276577 0.2561171736036028 0.2837568202189037 0.2415965497493744 0.2721711982573782 0.26305754695619854 0.27356812890086857 0.28497052511998583 0.29270052909851074 Testloss: 0.2924374173873827 
