CONFIGURATION: 
Training time: 1533 
lr: 0.002
wd: 0.001
d_model: 256
n_heads: 8
n_layers: 4
d_ff: 512
batch_size: 512
dropout: 0.19636596255669625
margin: 0.2010657848212919
epsilon: 1e-06
Train losses 
0.4410089492797852
0.32756415009498596
0.3058903068304062
0.32230154275894163
0.32015925347805024
0.3254030138254166
0.29945200085639956
0.305600181221962
0.2911491870880127
0.2874109670519829
0.29410121142864226
0.2703446552157402
0.2643431395292282
0.27535494714975356
0.2844438776373863
0.27576148957014085
0.2740394756197929
0.28114993721246717
0.27832290679216387
0.26327954679727555
0.27255290001630783
0.2529034301638603
0.24346775710582733
0.23211684226989746
0.2242386296391487
0.23354587107896804
0.25156782269477845
0.2375602677464485
0.24086748510599137
0.23182711750268936
Val losses 
0.22401690483093262
0.1060679703950882
0.1126728355884552
0.13563387095928192
0.14782127737998962
0.15172222256660461
0.1573491096496582
0.14202409982681274
0.1499144434928894
0.13878387212753296
0.14070628583431244
0.13962920010089874
0.145888552069664
0.1357928067445755
0.13995295763015747
0.1308950036764145
0.14231868088245392
0.14073559641838074
0.14792583882808685
0.13933899998664856
0.12651753425598145
0.1358402967453003
0.18721014261245728
0.19622431695461273
0.22903408110141754
0.14945736527442932
0.21538089215755463
0.17021407186985016
0.1510642170906067
0.22152234613895416
