CONFIGURATION: 
lr: 0.001
wd: 0.1
d_model: 128
n_heads: 16
n_layers: 3
d_ff: 512
batch_size: 32
dropout: 0.19569545071740224
margin: 0.40937148673558144
epsilon: 0.0001
Train losses 
0.3715029251796228
0.24892250299453736
0.2045986275430079
0.18420671555731033
0.16256309057827348
0.14261851252781022
0.12990349235909956
0.11842462503247791
0.10574894707511973
0.10340711697936059
0.10023814268686153
0.09362153126685707
0.08694565011947243
0.08559028594582169
0.08149240891690608
0.07591575044724676
0.07381635738743676
0.0670441803280954
0.06845753521279052
0.06932886171120184
0.06153215608663029
0.06226551074672628
0.06540467258956698
0.05851784679624769
0.053436582231963126
Val losses 
0.22817086546044602
0.22163756492367961
0.22028965816686027
0.21099161801108143
0.21165708949168524
0.20239709265399397
0.17275839913309665
0.21244191692063683
0.1685627649227778
0.17117333595167128
0.1547224258905963
0.17527944293984196
0.1474637591786552
0.17439476439827367
0.15726259755983688
0.14728145209843652
0.1466256072908117
0.145695618500835
0.15041230658167287
0.14626850004781755
0.12731705998119555
0.14309719936889514
0.13163445458600395
0.14871616504694285
0.11809486412165458
