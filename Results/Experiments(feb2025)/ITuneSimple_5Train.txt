CONFIGURATION: 
Learning rate: 0.00E+00 
margin: 0.24 
batch_size: 16 
number of layers: 3 
Train losses 
0.6560974445805621
0.5604091580234357
0.5162836104186613
0.5397983144468336
0.5134366692891762
0.5007936340659412
0.4623487275927814
0.4582050976468556
0.44831443856011577
0.42976757928506654
0.4153465023681299
0.40110764129837945
0.36867166988885225
0.34707513896387016
0.3391106909335549
0.3332027276950096
0.3248670087821448
0.32439206740749416
0.30923214132216437
0.2908492135023003
0.28776911256918264
0.28499329890777814
0.2824911724275617
0.27805542945861816
0.2728384441849011
0.6166716003859485
0.6181993908352322
0.6150901563741543
0.6225860257943471
0.6118744820356369
0.599611950914065
0.5964694879673146
0.6142686588896645
0.6229566762844722
0.5918429164974778
0.5935037534545969
0.5880218547803384
0.6032503099353225
0.5830843145096744
0.585226571559906
0.583820789831656
0.5893806211374424
0.2841048687696457
0.2757000423139996
0.3333986985462683
0.44338882918711064
0.49696483501681576
0.5444508751233419
0.5158270257490652
0.5241453742539441
0.514414518630063
0.50906857141742
0.5067411096007736
0.5054187931396343
0.4998836241386555
0.4991898512398755
0.490707426821744
0.4939555397740117
0.4952064127833755
0.49551369349161783
0.49045156902737086
0.491036550866233
0.6652801483869553
0.6721246466040611
0.6672620624303818
0.654623493552208
0.6457553505897522
0.6603094786405563
0.6560669243335724
0.6429185420274734
0.6388573795557022
0.6279082223773003
0.6488242149353027
0.6279678046703339
0.6312614008784294
0.6411639302968979
0.620943546295166
0.6312003210186958
0.6202041432261467
0.622772328555584
0.6185212284326553
0.6215010359883308
0.6148229688405991
0.6158554926514626
0.6114488616585732
0.6227817237377167
0.6153039857745171
0.41246336334281497
0.4040058739759304
0.3843815785867197
0.3711400989029143
0.3548929959535599
0.34742578897211285
0.333303580460725
0.32223128687452385
0.31147145882800775
0.3033260855409834
0.2948933889468511
0.2926469801752656
0.2886311888694763
0.2890212798560107
0.19278182199707738
0.20779197834156177
0.29563529094060265
0.3547706098468215
0.3222696849041515
0.29527639553502755
0.2824380158826157
0.2658087577532839
0.2507003684838613
0.2379950632099752
0.2263128916974421
0.21881193115755362
0.21178788802138082
0.206740143812365
0.20018803849816322
0.19939377995000945
0.18438742927930973
0.1876045424905088
0.18248961766560873
0.18557439482322446
0.1795940252089942
0.17411873075697157
0.17091177155574164
0.17361624124977323
0.1717405672426577
Val losses 
0.5025413948556651
0.6228826818258866
0.7905291234669478
0.9858446167862933
0.9895275131515835
0.9006878046885781
0.9393832134163898
0.8553737946178602
0.8332742325637652
0.7682159286478292
0.815374583783357
0.7298095397327258
0.7335290281668954
0.7301321744918823
0.6986688463584236
0.6668280207592509
0.7241205785585486
0.7212096219477446
0.6669679786847985
0.6712701825991921
0.6293753416641898
0.6440608742444411
0.6841712632904882
0.6470674360575883
0.6125561826902887
