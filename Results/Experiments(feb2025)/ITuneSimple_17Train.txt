CONFIGURATION: 
Learning rate: 0.00E+00 
margin: 0.65 
batch_size: 64 
number of layers: 1 
Train losses 
0.43487731107959043
0.43095960252814824
0.42428294795530813
0.4274104321444476
0.40959089475649374
0.41602669656276703
0.4034270387004923
0.38997787568304276
0.3952388675124557
0.40465196227585826
0.39310738024888214
0.39679688855453776
0.40259273604110435
0.3775142189529207
0.3840125521024068
0.3813038925329844
0.3800438164560883
0.3948577267152292
0.3904116815990872
0.3804437896719685
0.3792365231999644
0.3787726178213402
0.3739649774851622
0.36881867856891065
0.3855423416252489
0.5158991309227767
0.48685026985627633
0.4507211838607435
0.4147808272529531
0.3566474441024992
0.3271544778788531
0.30516462248784526
0.2821688505234542
0.25517397191789415
0.24293199446466235
0.2209977681990023
0.2151049945089552
0.21066485234984644
0.19045164463696657
0.18576051968115348
0.1839979682807569
0.18291765903985058
0.1738495268203594
0.16733315763650117
0.17012784900488676
0.1738462198663641
0.16344108934755677
0.1618244641356998
0.1624626006241198
0.16503308525791874
0.7836461351977454
0.7860877460903591
0.747736617481267
0.7233473504031146
0.7147124697764714
0.7008420021445663
0.6926499089709035
0.6716665022903019
0.6643270475996865
0.6505801899565591
0.6535906698968675
0.6285206961411017
0.6190919084681406
0.3436348216401206
0.3098849140935474
0.2661963832599145
0.21691439659507186
0.18913114766279857
0.16154993330990827
0.15129531307352914
0.13658389686434358
0.12566690505654723
0.1290735325327626
0.12413876608566002
0.11627873480319977
0.11047477683535328
0.1005070576513255
0.09531455150356999
0.09637873603238
0.09471371267680768
0.09081935258927169
0.08351986860787428
0.08395356662847378
0.08834669843867973
0.0834767708071956
0.08569748429236589
0.08081204725636376
0.07935603465195055
0.7054745234824993
0.6876081935785435
0.6319996539089415
0.5893618713926386
0.5303740561008453
0.4702755723838453
0.4352906462219026
0.4037944080653014
0.39236537295359153
0.35356878748646486
0.3450457000070148
0.3290526693617856
0.3195852099745362
0.29526624502959076
0.29338884397789283
0.2856403355245237
0.27500819442448793
0.2784406447852099
0.27468064383224206
0.27107091203883843
0.2577139205402798
0.26409796917880024
0.24538956152068245
0.3160342798749013
0.21213955999310338
0.15564081261851895
0.12488549908817705
0.11111252443559134
0.08857411502012566
0.08205793347598901
0.07669752096729492
0.065112847112008
0.05871919876159127
0.057810439714300096
0.049799854519651895
0.04418930394658402
0.045530459106858094
0.043327263820527206
0.0368732117227654
0.03704269135843462
0.03373408067359853
0.034867699410933164
0.03276092775944454
0.031322628259658813
0.029297051867887156
0.028411345917787125
0.02860314165478322
0.028197627403398057
0.5916582094298468
0.6033219778979266
0.5934881585615652
0.5973026169670953
0.5824988967842526
0.5936531729168362
0.5832757086665542
0.5908118718200259
0.5643601874510448
0.5683090214376096
0.5820434742503696
0.5862047888614513
0.5741596696553407
0.5753182318475512
0.5827801863352458
0.5583255233588043
0.5612716683635005
0.5684603909651439
0.560997293834333
0.5677946631555204
0.5729015471758666
0.5633119719999807
0.562130351419802
0.5530892780533543
0.5532212886545393
Val losses 
0.6856414207390377
0.7009998602526528
0.7028944098523685
0.6967542512076241
0.6930475948112351
0.6815966069698334
0.6834166177681514
0.655282084430967
0.6641497420413154
0.639001513166087
0.6225743549210685
0.670699553830283
0.6312517204454967
0.6570870716656957
0.6518121127571378
0.6863013708165714
0.6495489797421864
0.6692229541284698
0.6583226919174194
0.6463777720928192
0.664893239736557
0.6546711847186089
0.6382423064538411
0.6233481860586575
0.6540564298629761
