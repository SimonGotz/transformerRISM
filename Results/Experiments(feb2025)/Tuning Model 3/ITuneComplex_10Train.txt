CONFIGURATION: 
lr: 2e-05
wd: 0.0001
d_model: 512
n_heads: 16
n_layers: 3
d_ff: 512
batch_size: 16
dropout: 0.0886794625587048
margin: 0.7736612671467831
epsilon: 1e-05
Train losses 
0.4192686759763294
0.39861438715899433
0.3693030701743232
0.32596208442140506
0.29742226512343795
0.2608463986052407
0.2389655003945033
0.22313939542682082
0.21309645771980285
0.19835496346155804
0.1938243493989662
0.18424453547707312
0.17624747311627423
0.17410662163186957
0.17099595290643196
0.15238672693570454
0.16074291533894008
0.15554484658771092
0.15432413187291888
0.14525793923272026
0.14690410373387514
0.1427460233370463
0.14362364775604672
0.1441357120319649
0.14462771537127317
Val losses 
0.36780713692955347
0.3533564707507258
0.3301936538323112
0.3365225460218347
0.320961833777635
0.3167281363321387
0.32602849835934844
0.27892167360886283
0.3011366468408833
0.28026986808880516
0.28301562304082123
0.250228866805201
0.2729139177695565
0.2500663962053216
0.2625346961228744
0.281719622404679
0.26975779300150665
0.2667424036108929
0.2589367407819499
0.24692646394605222
0.25438314209813656
0.23517947067385134
0.2471741824046425
0.2639847869458406
0.23176289485848467
