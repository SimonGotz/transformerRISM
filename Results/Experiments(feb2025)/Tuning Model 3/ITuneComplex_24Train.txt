CONFIGURATION: 
lr: 1e-05
wd: 0.0001
d_model: 16
n_heads: 2
n_layers: 1
d_ff: 128
batch_size: 256
dropout: 0.35184318928077335
margin: 0.293248029849271
epsilon: 0.001
Train losses 
0.597457797238321
0.6035127693956549
0.5900732640064124
0.5796544028050972
0.5998998139843796
0.5797686053044868
0.5848484707601143
0.5984867558334813
0.5963528535582803
0.5865843025120822
0.5834513125997601
0.5728739156867518
0.5896854400634766
0.5919687946637472
0.5765124324596289
0.5841458223082803
0.5861897540814949
0.5680590985399304
0.5696749172427438
0.5832219828258861
0.5869222689758647
0.5795182648933295
0.5746583803133531
0.5816423874912839
0.5774041874842211
Val losses 
0.2650753791843142
0.2523631772824696
0.2537489128964288
0.2532724546534674
0.2702659262078149
0.26692019615854534
0.2569781520536968
0.25191935683999744
0.2628033012151718
0.2592502406665257
0.24433467643601553
0.2423564110483442
0.26330011231558664
0.25027800670691897
0.2789119482040405
0.2660485953092575
0.25318718382290434
0.2510891322578703
0.2677385083266667
0.26488778633730753
0.2652758913380759
0.2613927147218159
0.26095091870852877
0.2561413624456951
0.24480313701289041
