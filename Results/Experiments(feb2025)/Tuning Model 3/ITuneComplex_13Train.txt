CONFIGURATION: 
lr: 2e-05
wd: 0.1
d_model: 512
n_heads: 16
n_layers: 2
d_ff: 256
batch_size: 1024
dropout: 0.12070951275478192
margin: 0.6749766021666693
epsilon: 1e-08
Train losses 
0.4331844598054886
0.43783941492438316
0.4302448481321335
0.42028411477804184
0.4101978838443756
0.3761592246592045
0.3927192762494087
0.3830995336174965
0.3824640028178692
0.3766140341758728
0.3699296973645687
0.3604215085506439
0.36454689502716064
0.347203329205513
0.34165482223033905
0.3415299132466316
0.3406611420214176
0.34123721718788147
0.34971221163868904
0.3474685475230217
0.3374391235411167
0.3372919075191021
0.33702919632196426
0.335533507168293
0.3373912237584591
Val losses 
0.41327759623527527
0.41681429743766785
0.3999134600162506
0.356703519821167
0.38621917366981506
0.3762608468532562
0.3638307750225067
0.3535043001174927
0.38284212350845337
0.339438796043396
0.3445105254650116
0.3947050869464874
0.36054593324661255
0.3647981286048889
0.3630736470222473
0.3868645131587982
0.3532312512397766
0.38457974791526794
0.4312668442726135
0.3526915907859802
0.3553442358970642
0.33236172795295715
0.38012659549713135
0.3535040616989136
0.3443983495235443
