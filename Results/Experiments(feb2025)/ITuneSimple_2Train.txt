CONFIGURATION: 
Learning rate: 2.95E-04 
margin: 0.49 
batch_size: 64 
number of layers: 3 
Train losses 
0.6560974445805621
0.5604091580234357
0.5162836104186613
0.5397983144468336
0.5134366692891762
0.5007936340659412
0.4623487275927814
0.4582050976468556
0.44831443856011577
0.42976757928506654
0.4153465023681299
0.40110764129837945
0.36867166988885225
0.34707513896387016
0.3391106909335549
0.3332027276950096
0.3248670087821448
0.32439206740749416
0.30923214132216437
0.2908492135023003
0.28776911256918264
0.28499329890777814
0.2824911724275617
0.27805542945861816
0.2728384441849011
0.6166716003859485
0.6181993908352322
0.6150901563741543
0.6225860257943471
0.6118744820356369
0.599611950914065
0.5964694879673146
0.6142686588896645
0.6229566762844722
0.5918429164974778
0.5935037534545969
0.5880218547803384
0.6032503099353225
0.5830843145096744
0.585226571559906
0.583820789831656
0.5893806211374424
0.2841048687696457
0.2757000423139996
0.3333986985462683
0.44338882918711064
0.49696483501681576
0.5444508751233419
0.5158270257490652
0.5241453742539441
0.514414518630063
0.50906857141742
0.5067411096007736
0.5054187931396343
0.4998836241386555
0.4991898512398755
0.490707426821744
0.4939555397740117
0.4952064127833755
0.49551369349161783
0.49045156902737086
0.491036550866233
Val losses 
0.5224250789199557
0.5806625708937645
0.7238486515624183
0.7705043447869164
1.022507050207683
0.9478470236063004
0.9849717084850583
1.0004182904958725
0.9992601530892509
1.001786200063569
0.9925369152000972
1.005913251212665
0.9770666318280357
0.9590935621942792
1.0609122472149985
1.044559964111873
1.0003933225359236
1.009717475090708
1.0469118420566832
1.0915929887975966
