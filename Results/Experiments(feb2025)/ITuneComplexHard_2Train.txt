CONFIGURATION: 
lr: 2e-05
wd: 0.05
d_model: 512
n_heads: 16
n_layers: 2
d_ff: 512
batch_size: 32
dropout: 0.410392912012607
margin: 0.7885992632882616
epsilon: 1e-07
Train losses 
0.8807857316953165
0.8592254230269679
0.8561675879690382
0.8173608859380086
0.803324219915602
0.7895968585102646
0.9078245337362643
0.8843511676346815
0.8659408728281657
0.8530800110763974
0.8410428804379922
0.8450074107558639
0.8250916198447898
0.80777652087035
0.8092886105731681
0.7988741572256441
0.8012601057688395
0.8119813866085477
0.8022083969027908
0.7913246448393221
0.7823670186378338
0.7807364834679498
0.7854287273354
0.790692283489086
0.7909352156851027
Val losses 
0.3746430852956939
0.39208412693257916
0.40860268944188166
0.46768848310437117
0.4181385259879263
0.4939249837607668
0.45512172998043526
0.46480053663253784
0.4118016681127381
0.41375212057640676
0.42286390327570733
0.3916121475529252
0.38129908526152895
0.36888080547776136
0.39842522667165386
0.3717772513628006
0.35754172634660153
0.3747923416003846
0.36359632773357525
0.3907424211502075
0.3577414628183633
0.3752940051388322
0.3755490649164769
0.37330282022032824
0.39852337356199297
