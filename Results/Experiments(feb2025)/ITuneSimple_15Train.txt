CONFIGURATION: 
Learning rate: 3.69E-07 
margin: 0.88 
batch_size: 16 
number of layers: 3 
Train losses 
0.43487731107959043
0.43095960252814824
0.42428294795530813
0.4274104321444476
0.40959089475649374
0.41602669656276703
0.4034270387004923
0.38997787568304276
0.3952388675124557
0.40465196227585826
0.39310738024888214
0.39679688855453776
0.40259273604110435
0.3775142189529207
0.3840125521024068
0.3813038925329844
0.3800438164560883
0.3948577267152292
0.3904116815990872
0.3804437896719685
0.3792365231999644
0.3787726178213402
0.3739649774851622
0.36881867856891065
0.3855423416252489
0.5158991309227767
0.48685026985627633
0.4507211838607435
0.4147808272529531
0.3566474441024992
0.3271544778788531
0.30516462248784526
0.2821688505234542
0.25517397191789415
0.24293199446466235
0.2209977681990023
0.2151049945089552
0.21066485234984644
0.19045164463696657
0.18576051968115348
0.1839979682807569
0.18291765903985058
0.1738495268203594
0.16733315763650117
0.17012784900488676
0.1738462198663641
0.16344108934755677
0.1618244641356998
0.1624626006241198
0.16503308525791874
0.7836461351977454
0.7860877460903591
0.747736617481267
0.7233473504031146
0.7147124697764714
0.7008420021445663
0.6926499089709035
0.6716665022903019
0.6643270475996865
0.6505801899565591
0.6535906698968675
0.6285206961411017
0.6190919084681406
0.3436348216401206
0.3098849140935474
0.2661963832599145
0.21691439659507186
0.18913114766279857
0.16154993330990827
0.15129531307352914
0.13658389686434358
0.12566690505654723
0.1290735325327626
0.12413876608566002
0.11627873480319977
0.11047477683535328
0.1005070576513255
0.09531455150356999
0.09637873603238
0.09471371267680768
0.09081935258927169
0.08351986860787428
0.08395356662847378
0.08834669843867973
0.0834767708071956
0.08569748429236589
0.08081204725636376
0.07935603465195055
0.7054745234824993
0.6876081935785435
0.6319996539089415
0.5893618713926386
0.5303740561008453
0.4702755723838453
0.4352906462219026
0.4037944080653014
0.39236537295359153
0.35356878748646486
0.3450457000070148
0.3290526693617856
0.3195852099745362
0.29526624502959076
0.29338884397789283
0.2856403355245237
0.27500819442448793
0.2784406447852099
0.27468064383224206
0.27107091203883843
0.2577139205402798
0.26409796917880024
0.24538956152068245
Val losses 
0.6849620824274809
0.6546199643093606
0.6277354287064594
0.5817659813424815
0.55819302589997
0.5023277567780536
0.4807726061862448
0.47563051710958065
0.5137832175130429
0.45715395160343336
0.4832231666730798
0.4736965143162271
0.47079010709472324
0.4637579098991726
0.4435614233431609
0.49125655630360476
0.436776588274085
0.4391172494577325
0.47285585610762887
0.4449034193287725
0.4734204033146734
0.4798025670258895
0.4933769350466521
