CONFIGURATION: 
lr: 1e-05
wd: 0.05
d_model: 32
n_heads: 2
n_layers: 2
d_ff: 1024
batch_size: 1024
dropout: 0.0822319325550188
margin: 0.6253317391071544
epsilon: 1e-06
Train losses 
0.6276639774441719
0.6142967194318771
0.616801418364048
0.5980030074715614
0.6064274683594704
0.585496261715889
0.8703637942671776
0.8839505836367607
0.8791281953454018
0.8791858553886414
0.8629006743431091
0.8481323421001434
0.850256159901619
0.8498572334647179
0.8415326401591301
0.8423333689570427
0.8341707661747932
0.8275613710284233
0.834845557808876
0.8409727066755295
0.8262098431587219
0.831333301961422
0.8044179007411003
0.8275133073329926
0.8215249702334404
Val losses 
0.5272688269615173
0.525355875492096
0.5412768721580505
0.5436384081840515
0.529767632484436
0.5546731948852539
0.5217363834381104
0.5281935334205627
0.5257377028465271
0.5123792290687561
0.5353439450263977
0.5467962026596069
0.5038290023803711
0.5209970474243164
0.5096900463104248
0.5097399353981018
0.5314556360244751
0.5172678828239441
0.5238710045814514
0.507942259311676
0.5381426215171814
0.5304215550422668
0.48946064710617065
0.5359188318252563
0.5053746700286865
