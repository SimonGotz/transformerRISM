CONFIGURATION: 
lr: 1e-06
wd: 0.005
d_model: 64
n_heads: 8
n_layers: 3
d_ff: 128
batch_size: 256
dropout: 0.12350452759440378
margin: 0.2730240234711493
epsilon: 1e-05
Train losses 
0.38681348977666913
0.3807706850947756
0.38816990996852063
0.3861801136623729
0.3910729668357156
0.3812014036106341
0.5852597933827024
0.5893168611959978
0.5731077040686752
0.5862706520340659
0.5833385550614559
0.5729571960189126
0.5756307352672924
0.5801326135794321
0.5839139566277013
0.5869478185971578
0.570906788110733
0.5843906366463864
0.5778442305145841
Val losses 
0.2623949327639171
0.2777558075530188
0.29002500006130766
0.2755565345287323
0.26716657621519907
0.2743473159415381
0.2797616102865764
0.27910350475992474
0.2758130601474217
0.2756676503590175
0.2725220280034201
0.2610754796436855
0.26444078981876373
0.25835247124944416
0.2801019868680409
0.27225446913923534
0.2739881042923246
0.27620006459099905
0.2958126536437443
