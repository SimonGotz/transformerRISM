CONFIGURATION: 
lr: 1e-05
wd: 0.05
d_model: 32
n_heads: 16
n_layers: 4
d_ff: 2048
batch_size: 32
dropout: 0.7099591833933587
margin: 0.2885881533013473
epsilon: 1e-06
Train losses 
0.5979638318220775
0.581277538449676
0.5890423680345217
0.5529958820453397
0.5600840720865462
0.5427219556437598
0.5129271693803646
0.5112326678854447
0.5040543124631599
0.5027360864259579
0.4953440502837852
0.4883761113440549
0.4875547545927542
0.4804822729141624
0.47406003844958766
0.47208368800304557
0.4823380247310356
0.47768983598108644
0.4747510270939933
0.4691445906956991
0.46217488950049435
0.4653275568176199
0.4818914932785211
0.47210776000111193
0.47353106715061044
Val losses 
0.22467443163980516
0.20172933763579318
0.20814238567101329
0.21287858028683745
0.17582238086482935
0.17708311373727365
0.17594715848303677
0.17088069766759872
0.16480381917535214
0.15626288061602073
0.15887966686696336
0.1570334965199755
0.15297318642076693
0.1510412077370443
0.15521439504727982
0.15615907246083544
0.15555380376284583
0.15759876710281037
0.15668956778551402
0.1565432395589979
0.15790507877082155
0.162056059560232
0.1604297842111504
0.16107471823169475
0.1610983729101064
