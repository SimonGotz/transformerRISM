CONFIGURATION: 
Training time: 2126 
lr: 2e-05
wd: 0.0001
d_model: 512
n_heads: 16
n_layers: 3
d_ff: 512
batch_size: 16
dropout: 0.0886794625587048
margin: 0.7736612671467831
epsilon: 1e-05
Train losses 
0.4073721444165265
0.38834506615444464
0.34984605389612694
0.30362906146932533
0.2637148788681737
0.24265276745513634
0.206719602699633
0.2031953630623994
0.16907277316958816
0.16016409121177816
0.15824163909311648
0.14796883817072268
0.14361556304825676
0.13061627745628357
0.1291903606167546
0.11548214621014065
0.11346501674917009
0.1132443877281966
0.10907851104383115
0.09847092860274845
0.09878691412784435
0.09347592702618351
0.08962426671275386
0.0846369077761968
0.08783956429472677
0.08307585859740221
0.0758780465081886
0.0748836429030807
0.07064726981851789
0.06916756508526979
Val losses 
0.38576239347457886
0.3225265145301819
0.2736116349697113
0.24110324680805206
0.217090904712677
0.18423525989055634
0.1751786470413208
0.15324540436267853
0.15028928220272064
0.14331789314746857
0.15071457624435425
0.1274137943983078
0.11672092974185944
0.12074805051088333
0.10504839569330215
0.10627325624227524
0.08910860121250153
0.10094781965017319
0.09536691755056381
0.08855428546667099
0.07541778683662415
0.07994690537452698
0.07119495421648026
0.08241686969995499
0.049641191959381104
0.06077723205089569
0.07685858011245728
0.04646466299891472
0.05951201543211937
0.04663849622011185
