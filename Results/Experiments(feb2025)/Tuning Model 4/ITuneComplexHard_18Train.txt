CONFIGURATION: 
Training time: 309 
lr: 0.0001
wd: 0.005
d_model: 64
n_heads: 8
n_layers: 2
d_ff: 2048
batch_size: 2048
dropout: 0.0802204170297543
margin: 0.32663754317361593
epsilon: 0.1
Train losses 
0.5769971311092377
0.5643588453531265
0.5726276934146881
0.5588782727718353
0.5516128689050674
0.5621278882026672
0.5419935137033463
0.553597629070282
0.5614901483058929
0.5541256368160248
0.55536849796772
0.544663280248642
0.5411005318164825
0.5259909778833389
0.550810843706131
0.5327296704053879
0.5492346733808517
0.5366907566785812
0.5382626056671143
0.5162254273891449
0.5425019711256027
0.5265637934207916
0.5423921793699265
0.5383014976978302
0.539446234703064
Val losses 
0.25428280234336853
0.23783054947853088
0.22720323503017426
0.23141899704933167
0.24335463345050812
0.2249048352241516
0.25631532073020935
0.2357179969549179
0.2472107857465744
0.2329138070344925
0.2390197217464447
0.24086560308933258
0.24051737785339355
0.24817657470703125
0.22852806746959686
0.2309848517179489
0.23338264226913452
0.2089918702840805
0.22778242826461792
0.2233716994524002
0.23930077254772186
0.2259235829114914
0.2241748571395874
0.23268651962280273
0.2290927618741989
