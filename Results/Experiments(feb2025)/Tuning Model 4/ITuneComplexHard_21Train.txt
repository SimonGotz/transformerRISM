CONFIGURATION: 
Training time: 4617 
lr: 1e-06
wd: 0.0001
d_model: 512
n_heads: 4
n_layers: 1
d_ff: 128
batch_size: 2048
dropout: 0.523287874976804
margin: 0.1023540660708702
epsilon: 1e-05
Train losses 
0.5373538732528687
0.5410953909158707
0.5257162153720856
0.5183657109737396
0.526266872882843
0.524044007062912
0.5354939848184586
0.5467629283666611
0.551831528544426
0.5144447609782219
0.5274888873100281
0.5448478758335114
0.5356418192386627
0.5498692095279694
0.5248803049325943
0.5204298943281174
0.5472733378410339
0.5245650410652161
0.5021215155720711
0.5285176932811737
0.5330679565668106
0.5289710760116577
0.5340156257152557
0.5290583223104477
0.5637716799974442
Val losses 
0.2706848084926605
0.2709466218948364
0.2835958003997803
0.3322356343269348
0.3116832673549652
0.32139575481414795
0.27900296449661255
0.28176379203796387
0.2810266315937042
0.3038212060928345
0.2974121570587158
0.3066371977329254
0.2972757816314697
0.28413379192352295
0.27858224511146545
0.2815130650997162
0.296372652053833
0.285579115152359
0.31187623739242554
0.2958434224128723
0.29534924030303955
0.2951492965221405
0.2881527841091156
0.2783866822719574
0.3098664879798889
