CONFIGURATION: 
Learning rate: 0.00E+00 
margin: 0.59 
batch_size: 16 
number of layers: 2 
Train losses 
0.43487731107959043
0.43095960252814824
0.42428294795530813
0.4274104321444476
0.40959089475649374
0.41602669656276703
0.4034270387004923
0.38997787568304276
0.3952388675124557
0.40465196227585826
0.39310738024888214
0.39679688855453776
0.40259273604110435
0.3775142189529207
0.3840125521024068
0.3813038925329844
0.3800438164560883
0.3948577267152292
0.3904116815990872
0.3804437896719685
0.3792365231999644
0.3787726178213402
0.3739649774851622
0.36881867856891065
0.3855423416252489
0.5158991309227767
0.48685026985627633
0.4507211838607435
0.4147808272529531
0.3566474441024992
0.3271544778788531
0.30516462248784526
0.2821688505234542
0.25517397191789415
0.24293199446466235
0.2209977681990023
0.2151049945089552
0.21066485234984644
0.19045164463696657
0.18576051968115348
0.1839979682807569
0.18291765903985058
0.1738495268203594
0.16733315763650117
0.17012784900488676
0.1738462198663641
0.16344108934755677
0.1618244641356998
0.1624626006241198
0.16503308525791874
Val losses 
0.6766288446343464
0.6057579724685006
0.578715295376985
0.5917053911996925
0.49947575771290326
0.534677313980849
0.5068999578123508
0.5223227433536364
0.4704297029453775
0.4832353993602421
0.47835185009500253
0.4794934570789337
0.44520266729852426
0.4864269285098366
0.4634920957295791
0.4371990118337714
0.4417353192101354
0.39225927176682845
0.44603005725404493
0.42643934695617014
0.4078909526700559
0.35252947392671
0.4036772575067437
0.41533877331277597
0.41588788162107054
