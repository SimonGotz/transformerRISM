CONFIGURATION: 
lr: 0.002
wd: 0.005
d_model: 128
n_heads: 16
n_layers: 2
d_ff: 128
batch_size: 16
dropout: 0.34807687038067203
margin: 0.6521555861832367
epsilon: 1e-08
Train losses 
0.7354384228035256
0.6246284429673795
0.5179899348153009
0.49162374138832093
0.47775172618804157
0.4621008296807607
0.45304054043911124
0.42473587090218506
0.40854353717079867
0.41126550998952655
0.4002613483203782
0.38597012487826526
0.36632957916568826
0.37560839357751385
0.3595799496880284
0.33685350183535506
0.3335777384263498
0.3235343196601779
0.3216443523488663
0.3191392466425896
0.31614649199225286
0.3143441650050658
0.29882360487072557
Val losses 
0.44018482060536096
0.5379828041014464
0.5610891230728315
0.5532911615527194
0.5884544543598009
0.49989601632823116
0.5274941051783769
0.5435101733259533
0.5434487360975017
0.46078172688898833
0.451531063085017
0.43829122729923414
0.4562038613402325
0.4208306168732436
0.45361322034960205
0.42641191106775533
0.43285878080388773
0.40086644900881724
0.38303999848987746
0.36571060639360675
0.3927871522696122
0.4180497900299404
0.43765555762726327
