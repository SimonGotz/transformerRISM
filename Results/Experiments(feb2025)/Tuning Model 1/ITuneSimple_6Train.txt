CONFIGURATION: 
lr: 0.001
wd: 0.05
d_model: 32
n_heads: 8
n_layers: 2
d_ff: 512
batch_size: 64
dropout: 0.3469276146882739
margin: 0.3747042873980685
epsilon: 1e-06
Train losses 
0.5525899191697439
0.4684373539906961
0.43847500571498166
0.4259213403419212
0.4146263939362985
0.4027145281985954
0.39039416004110267
0.3731180184417301
0.359115344065207
0.3539798074298435
0.3360519135439837
0.32990036805470785
0.3165789151633227
0.317857849156415
0.3137097662245786
0.29758166604571873
0.29304643792134744
0.2872679552546254
0.28238408234384327
0.28272216695326347
0.28387570756453057
0.28110658990012277
0.2799016226221014
0.276497596612683
0.27134474626293886
Val losses 
0.29261258404169765
0.28206517919898033
0.28621979004570414
0.30640504296336857
0.2847115120717457
0.28031492073621067
0.29364748139466557
0.3016561535852296
0.28384150617889
0.280591680003064
0.28389683099729673
0.2989662289619446
0.2935088745185307
0.2791236617735454
0.2895958700350353
0.3040537206189973
0.28878816057528767
0.28140719128506525
0.26342418576989857
0.24694977487836564
0.26513826474547386
0.2614904668714319
0.27302180816020283
0.25172350502439905
0.27941485441156794
