CONFIGURATION: 
lr: 1e-06
wd: 0.0001
d_model: 128
n_heads: 2
n_layers: 1
d_ff: 128
batch_size: 64
dropout: 0.02405126680836647
margin: 0.7633904151056753
epsilon: 1e-05
Train losses 
0.6592322360586237
0.6620807594723171
0.6495218400601988
0.6550558544971324
0.6375351883746959
0.6440480051217256
0.6620445397165087
0.6567389327066916
0.6468465782977917
0.6509520574852272
0.6636440113738731
0.647313713365131
0.6735238635981524
0.660467165046268
0.6625965431884483
0.6477962957488166
0.6573440891725045
0.6465150950131593
0.6447211409056628
0.6692833439067558
0.649045706236804
0.6531840635670556
Val losses 
0.676971227994987
0.6905822189790862
0.6802037869180951
0.6899018713406154
0.6609791006360736
0.6554983385971614
0.6596718983990806
0.6865154440913882
0.6623725976262774
0.6453512949602944
0.675021863409451
0.7090962869780404
0.7056984944002969
0.6762452466147286
0.6710006701094764
0.6982801152127129
0.644300075513976
0.6840341687202454
0.6642101013234684
0.6882464970861163
0.6893836770738874
0.6989635399409703
