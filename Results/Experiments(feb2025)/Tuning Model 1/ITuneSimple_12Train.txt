CONFIGURATION: 
lr: 2e-06
wd: 0.005
d_model: 16
n_heads: 2
n_layers: 4
d_ff: 2048
batch_size: 32
dropout: 0.1170425249151224
margin: 0.29478331917910705
epsilon: 1e-06
Train losses 
0.49830880143024303
0.47328958572060975
0.41433388198967336
0.3707384576952016
0.3322181901446095
0.3227473345067766
0.31517637730748566
0.30356737518752064
0.3015120487522196
0.3032066583081528
0.30057399377778726
0.3020144835666374
0.30224426526714254
0.29641041590107814
0.2913305259413189
0.2942671039590129
0.29006473852528464
0.29425572013413465
0.29118966311216354
0.29233474792153746
0.2906835639918292
0.29377486805121106
0.2856483752528826
0.29556632185423815
0.29228830917014015
Val losses 
0.32399186413539083
0.320299100980424
0.27194335729929436
0.24356427501168168
0.23246934016545615
0.23378289634721322
0.22106483447970005
0.23516840657644106
0.23562640343841754
0.23195782669803552
0.22974525104489243
0.23190105797951682
0.23276339941903165
0.23102301620600516
0.22371224353187963
0.2344259478543934
0.23256713125789374
0.2370223089268333
0.2264129469792048
0.23044526498568685
0.23654428281282125
0.23343872684135772
0.21611575756156653
0.23172220901439064
0.23658176498454914
