CONFIGURATION: 
lr: 0.002
wd: 0.1
d_model: 128
n_heads: 16
n_layers: 3
d_ff: 2048
batch_size: 256
dropout: 0.11154178473174162
margin: 0.3685745909312769
epsilon: 1e-05
Train losses 
0.34015190240108606
0.2673079412091862
0.2219502799438708
0.19485817804481043
0.17420854938752722
0.15279064350055926
0.13921782780777325
0.12931588766249744
0.11530188064683568
0.1144632597764333
0.10156406213839848
0.09390568168777408
0.08549793685475986
0.08211915477207213
0.07802272000999162
0.06669070571660995
0.07286273981585648
0.06738464455261375
0.06337575591874844
0.061768054510607864
0.058244417223966484
0.05655419160470818
0.051416677168824455
0.050015046120141494
0.0523530107668855
Val losses 
0.24400261683123453
0.24523113667964935
0.25548309300627026
0.32065186330250334
0.26228819148881094
0.24363615896020616
0.25654932643686024
0.22538759452956064
0.22277871625764029
0.22353417319910868
0.2307077114071165
0.21719571948051453
0.22707350764955794
0.1942257583141327
0.18516263152871812
0.18770559344972884
0.18820527621677943
0.17942396657807486
0.17412645263331278
0.17924409891877854
0.16967130984578813
0.18211759201117925
0.19387987681797572
0.18720452061721257
0.18595554786069052
