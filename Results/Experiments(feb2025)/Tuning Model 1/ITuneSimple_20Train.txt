CONFIGURATION: 
lr: 1e-05
wd: 0.0001
d_model: 32
n_heads: 16
n_layers: 1
d_ff: 256
batch_size: 1024
dropout: 0.32664538041332264
margin: 0.7067889096433118
epsilon: 0.001
Train losses 
0.8941161632537842
0.8853199928998947
0.8800643607974052
0.8752279654145241
0.8670502603054047
0.8559353947639465
0.8799601569771767
0.8946579471230507
0.8632159754633904
0.8663660883903503
0.8823413327336311
0.8614088147878647
0.8816305473446846
0.8583179414272308
0.8536374494433403
0.8739798814058304
0.8808034211397171
0.8666845709085464
0.880065955221653
0.8744129985570908
0.8778105825185776
0.8617237508296967
0.8628831431269646
0.8760509267449379
0.8719983473420143
Val losses 
0.5684069395065308
0.5516708493232727
0.5393052101135254
0.5525702238082886
0.591252863407135
0.5708933472633362
0.5355737209320068
0.5340964794158936
0.5448998212814331
0.5355867743492126
0.53630530834198
0.5702856183052063
0.5302748084068298
0.5746131539344788
0.566084623336792
0.558418869972229
0.5495452880859375
0.49659106135368347
0.5668963193893433
0.5678293704986572
0.5531656742095947
0.5963469743728638
0.5568662285804749
0.5356133580207825
0.5074899792671204
