CONFIGURATION: 
lr: 0.0002
wd: 0.0001
d_model: 32
n_heads: 4
n_layers: 3
d_ff: 2048
batch_size: 128
dropout: 0.10448039772695923
margin: 0.1784620381439243
epsilon: 1e-05
Train losses 
0.2799883822896587
0.1650442399862987
0.13691569886990448
0.12070904327417488
0.11753864742037076
0.10560675643718065
0.22267943487238528
0.2076078708936919
0.20267420962675295
0.20051118331169016
0.19769653789143063
0.19614994281263493
0.1947847876086164
0.19646655957200634
0.19264274991270322
0.18921980706613456
0.18867985414924907
0.18971218324419278
0.1886733605790494
0.1856612084961649
0.18553933123154426
0.18702307432445128
0.18622429557700657
0.18529514479103373
0.18096331184479728
Val losses 
0.18613543148551667
0.17666821288211004
0.17723648995161057
0.17537048991237367
0.1696901736514909
0.17393673530646733
0.13698997987168177
0.1395460803593908
0.13976212378059114
0.1368014413331236
0.14228105332170213
0.1396376086132867
0.13934555649757385
0.14029702437775476
0.140111329300063
0.13699837986912047
0.1390880878482546
0.14184514113834926
0.14027808659843036
0.13901398756674357
0.1377390090908323
0.14189992951495306
0.13717289907591684
0.13838737457990646
0.13564706008349145
