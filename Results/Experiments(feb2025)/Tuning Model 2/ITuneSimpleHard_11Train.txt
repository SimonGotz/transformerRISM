CONFIGURATION: 
Training time: 775 
lr: 2e-05
wd: 0.05
d_model: 512
n_heads: 2
n_layers: 3
d_ff: 128
batch_size: 2048
dropout: 0.7231909231317802
margin: 0.23174790366994982
epsilon: 1e-06
Train losses 
0.5735500752925873
0.5739758163690567
0.5609485656023026
0.5847982168197632
0.5757637023925781
0.5566507428884506
0.5559146106243134
0.5561861246824265
0.5615527629852295
0.5477446764707565
0.5457727611064911
0.5705588459968567
0.5691971778869629
0.5554104149341583
0.5772715210914612
0.5673463642597198
0.5665141642093658
0.5522426515817642
0.5608611404895782
0.5690236240625381
Val losses 
0.37373693049887846
0.34877858813681
0.3801584679136271
0.3427065424761986
0.3459268798704472
0.3621076514822673
0.36058784007768513
0.33690755422665325
0.36028052509829267
0.383665997295493
0.3720547600348919
0.3668212638166645
0.364846333885502
0.3637953379036993
0.3716665120591092
0.37366802791593395
0.34725795160558404
0.35556083907957403
0.3783342298593217
0.3894824515414328
