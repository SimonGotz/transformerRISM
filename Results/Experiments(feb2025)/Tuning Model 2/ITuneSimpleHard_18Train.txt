CONFIGURATION: 
lr: 2e-05
wd: 0.0005
d_model: 16
n_heads: 2
n_layers: 1
d_ff: 512
batch_size: 256
dropout: 0.6801576033622867
margin: 0.8666858135845927
epsilon: 0.01
Train losses 
1.0051229000091553
0.9919797471075347
1.0191991058262913
1.0217780579220166
1.0023176579764395
1.0163197535457034
1.0140922322417751
1.0306712930852717
1.0250508424007532
1.0075894395510356
1.0113533355972983
1.0342866543567542
1.0111204584439595
1.003562260757793
1.019123937144424
1.018958239844351
1.0179969469706218
1.0118970329111272
1.0068363746007283
1.0140244057684233
1.0141960834011887
1.005985433405096
1.009050569751046
1.0216312336199211
Val losses 
0.7705246039799282
0.7818509255136762
0.7938613380704608
0.7675903780119759
0.798153817653656
0.7646698270525251
0.7683699301310948
0.7691697307995388
0.768269922052111
0.7870620659419468
0.7772113510540554
0.7730006490434919
0.8013167040688651
0.7618071522031512
0.7825200472559247
0.8002747297286987
0.7871175578662327
0.7900007367134094
0.7758005516869682
0.7923736146518162
0.7717881032398769
0.7731250439371381
0.7839706454958234
0.7934232013566154
