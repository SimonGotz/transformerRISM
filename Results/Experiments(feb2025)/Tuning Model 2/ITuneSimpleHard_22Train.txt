CONFIGURATION: 
lr: 0.001
wd: 0.0001
d_model: 512
n_heads: 2
n_layers: 2
d_ff: 1024
batch_size: 64
dropout: 0.4196620065765719
margin: 0.17303594458368232
epsilon: 1e-05
Train losses 
0.48769330183664955
0.375591731733746
0.3383543836849707
0.37326255407598286
0.3801973105580718
0.4040193304971412
0.3868549860186047
0.37022388930673955
0.3606061225688016
0.354781323340204
0.3478004425764084
0.34459012073499184
0.34161871130819677
0.31926517641102825
0.335199577499319
0.32140658685454615
0.3250976113257585
0.3119872524782463
0.313683565236904
0.31008779152675914
0.305473342317122
0.30352370816248436
0.29834546226042286
0.30452949106693267
0.29862576734136653
Val losses 
0.34674637179289547
0.3998291737266949
0.5286365374922752
0.5281128180878503
0.5403265921132905
0.36202840134501457
0.3057899533637932
0.27734243710126194
0.3492167027933257
0.32697090400116785
0.2964231978569712
0.3460343357707773
0.31223891994782854
0.26366754727704184
0.383968180843762
0.3519878701439926
0.26743131556681227
0.3472151405044964
0.2768771307809012
0.25382316591484205
0.2592675755066531
0.2976215561585767
0.28791007825306486
0.29542237041252
0.30336352810263634
