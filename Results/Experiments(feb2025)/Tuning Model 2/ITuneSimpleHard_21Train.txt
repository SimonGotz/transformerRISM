CONFIGURATION: 
lr: 1e-05
wd: 0.05
d_model: 256
n_heads: 16
n_layers: 1
d_ff: 512
batch_size: 1024
dropout: 0.4235216594060483
margin: 0.17699938586135333
epsilon: 1e-05
Train losses 
0.562515377998352
0.5532627701759338
0.5476086214184761
0.5491036549210548
0.547707736492157
0.5434867069125175
0.5499898120760918
0.5545453876256943
0.5585605651140213
0.5795029029250145
0.5518939569592476
0.5735888108611107
0.5768480226397514
0.5574200451374054
0.5536148846149445
0.563176728785038
0.5625396817922592
0.5640937238931656
0.5686946734786034
0.564689502120018
0.5738522559404373
0.5681831985712051
0.566382884979248
0.5552452728152275
0.5626359954476357
Val losses 
0.26552271842956543
0.24886077642440796
0.24055252969264984
0.2618466317653656
0.24900951981544495
0.22726191580295563
0.24002741277217865
0.25575610995292664
0.25497686862945557
0.24618229269981384
0.26852333545684814
0.2671717405319214
0.24179045855998993
0.27018439769744873
0.24636036157608032
0.23952659964561462
0.26982057094573975
0.23683443665504456
0.2619699239730835
0.2407017946243286
0.24305732548236847
0.27230554819107056
0.2352907508611679
0.22238828241825104
0.2480754554271698
