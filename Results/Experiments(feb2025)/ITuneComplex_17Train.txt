CONFIGURATION: 
lr: 2e-06
wd: 0.005
d_model: 1024
n_heads: 2
n_layers: 4
d_ff: 1024
batch_size: 64
dropout: 0.6438444783510546
margin: 0.2068858527134509
epsilon: 1e-05
Train losses 
0.48713050683339437
0.49211638194543345
0.49897853047759444
0.4948861832971926
0.5000859136934633
0.4885713193151686
0.4995405276616414
0.49209367522486935
0.5003585497538249
0.4929663587499548
0.4942205367264924
0.48449596299065484
0.4967576018086186
0.4911559621493022
0.5012476651756852
0.4975094627451014
0.487316358089447
0.4890281774379589
0.4864636010593838
0.4937088745611685
0.49536968778680873
0.49206331394336844
0.486953858534495
0.4745518578423394
0.48013064287326956
Val losses 
0.5375826529094151
0.45004851850015776
0.4406356178224087
0.48463630782706396
0.49362748754876
0.5016430887792792
0.48068997051034656
0.5607528558799199
0.4736690047596182
0.4765486397913524
0.39920896557824953
0.4734356052109173
0.4951836402927126
0.4788905105420521
0.5228772392230374
0.44266544708183836
0.5111832395195961
0.5097494258412293
0.4859703869691917
0.4785847956580775
0.4723495878279209
0.5476528847856181
0.45935370879513876
0.4265581002192838
0.4888674483767578
