CONFIGURATION: 
lr: 0.001
wd: 0.01
d_model: 256
n_heads: 16
n_layers: 2
d_ff: 1024
batch_size: 256
dropout: 0.36561686564957396
margin: 0.2828951125890815
epsilon: 1e-08
Train losses 
0.47703108101180103
0.3411617396455823
0.27169264582070435
0.21813327390136142
0.198085192026514
0.18054487578796619
0.16807634699525256
0.15795467974561633
0.14995662077809824
0.14806145745696445
0.1352234369877613
0.14059061805407205
0.13212523157849457
0.12886026704853232
0.11971428967786557
0.12420702861113982
0.11578159959930362
0.11959228845256747
0.11471692743626508
0.11236756453008363
0.10846229766805966
0.11052326180718162
0.1045385892644073
0.10772735580350414
0.10356823287226936
Val losses 
0.2219025194644928
0.39901173966271536
0.3737294716494424
0.35294403774397715
0.2904143737895148
0.29680057082857403
0.25766595559460775
0.22963860843862807
0.23908301975045884
0.218602254986763
0.27479378453322817
0.23163205172334397
0.24109056804861342
0.22775758164269583
0.23258498736790248
0.23471741591181075
0.23005822513784682
0.24908049404621124
0.20343573604311263
0.22173665038176946
0.22837557750088827
0.21245259046554565
0.23952691682747432
0.2363527970654624
0.19555516753877913
