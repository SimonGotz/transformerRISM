CONFIGURATION: 
lr: 2e-06
wd: 0.0001
d_model: 512
n_heads: 8
n_layers: 2
d_ff: 1024
batch_size: 512
dropout: 0.670126994478161
margin: 0.15685754179800251
epsilon: 1e-06
Train losses 
0.5340088102966547
0.5181610491126776
0.5340833514928818
0.5305626466870308
0.5335663985460997
0.5315601415932178
0.5304232034832239
0.5306736547499895
0.5176413878798485
0.5364624597132206
0.5329424645751715
Val losses 
0.22658705711364746
0.21681687235832214
0.20841712752978006
0.25234899421532947
0.21962012350559235
0.2268119901418686
0.24134612083435059
0.2191354235013326
0.23317890365918478
0.24179048836231232
0.2512548665205638
